<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Getting started with the ebnm package • ebnm</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="Getting started with the ebnm package">
<meta property="og:description" content="ebnm">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    

    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">ebnm</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">1.1-0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">Home</a>
</li>
<li>
  <a href="../reference/index.html">Functions</a>
</li>
<li>
  <a href="../articles/ebnm.html">Getting Started</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/stephenslab/ebnm" class="external-link">Source</a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Getting started with the ebnm package</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/stephenslab/ebnm/blob/HEAD/vignettes/ebnm.Rmd" class="external-link"><code>vignettes/ebnm.Rmd</code></a></small>
      <div class="hidden name"><code>ebnm.Rmd</code></div>

    </div>

    
    
<div class="section level2">
<h2 id="the-empirical-bayes-normal-means-problem">The empirical Bayes normal means problem<a class="anchor" aria-label="anchor" href="#the-empirical-bayes-normal-means-problem"></a>
</h2>
<p>Given <span class="math inline">\(n\)</span> observations <span class="math inline">\(x_i\)</span> with known standard deviations <span class="math inline">\(s_i\)</span>, <span class="math inline">\(i = 1,
\dots, n\)</span>, the normal means model <span class="citation">(Robbins 1951; Efron and Morris 1972; Stephens 2017;
Bhadra et al. 2019; Johnstone 2019; Sun 2020)</span> is <span class="math display">\[\begin{equation}
x_i \overset{\text{ind.}}{\sim} \mathcal{N}(\theta_i, s_i^2),
\end{equation}\]</span> with the unknown (true) means <span class="math inline">\(\theta_i\)</span> to be estimated. Here and
throughout, we use <span class="math inline">\(\mathcal{N}(\mu,
\sigma^2)\)</span> to denote the normal distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. The maximum-likelihood estimate
of <span class="math inline">\(\theta_i\)</span> is, of course, <span class="math inline">\(x_i\)</span>. The empirical Bayes (EB) approach to
inferring <span class="math inline">\(\theta_i\)</span> attempts to
improve upon the maximum-likelihood estimate by “borrowing information”
across observations, exploiting the fact that each observation contains
information not only about its respective mean, but also about how the
means are collectively distributed <span class="citation">(Robbins 1956;
Morris 1983; Efron 2010; Stephens 2017)</span>. Specifically, the EB
approach assumes that <span class="math display">\[\begin{equation}
\theta_i \overset{\text{ind.}}{\sim} g \in \mathcal{G},
\end{equation}\]</span> where <span class="math inline">\(g\)</span> is
a distribution to be estimated from the data, typically chosen from
among some family of distributions <span class="math inline">\(\mathcal{G}\)</span> that is specified in advance.
(Note that although <span class="math inline">\(\mathcal{G}\)</span>
must be specified in advance, it can be arbitrarily flexible.)</p>
<p>The empirical Bayes normal means model is therefore fit by first
using all of the observations to estimate <span class="math inline">\(g
\in \mathcal{G}\)</span>, then using the estimated distribution <span class="math inline">\(\hat{g}\)</span> to compute posteriors for each
mean <span class="math inline">\(\theta_i\)</span>. Commonly, <span class="math inline">\(g\)</span> is estimated via maximum-likelihood, in
which case the EB approach consists of the following:</p>
<ol style="list-style-type: decimal">
<li><p>Find <span class="math inline">\(\hat{g} := \rm{argmax}_{g
\,\in\, \mathcal{G}} L(g)\)</span>, where <span class="math inline">\(L(g)\)</span> denotes the marginal likelihood,
<span class="math display">\[\begin{equation}
L(g) :=  p({\mathbf x} \mid g, {\mathbf s}) =
\prod_{i=1}^n \textstyle
\int p(x_i \mid \theta_i, s_i) \, g(\theta_i) \, d\theta_i,
\end{equation}\]</span> and we define <span class="math inline">\({\mathbf x} := (x_1, \ldots, x_n)\)</span>, <span class="math inline">\({\mathbf s} := (s_1, \ldots,
s_n)\)</span>.</p></li>
<li><p>Compute posterior distributions <span class="math display">\[\begin{equation}
p(\theta_i \mid x_i, s_i, \hat{g}) \propto
\hat{g}(\theta_i) \, p(x_i \mid \theta_i, s_i),
\end{equation}\]</span> and/or summaries (posterior means, variances,
etc.).</p></li>
</ol>
<p>We refer to this two-step process as “solving the EBNM problem.” The
<code>ebnm</code> package provides a unified interface for efficiently
solving the EBNM problem using a wide variety of prior families <span class="math inline">\(\mathcal{G}\)</span>. For some prior families, it
leverages code from existing packages; for others, it implements new
model fitting algorithms with a mind toward speed and robustness. In
this vignette, we demonstrate usage of the <code>ebnm</code> package in
an analysis of weighted on-base averages for Major League Baseball
players.</p>
</div>
<div class="section level2">
<h2 id="application-weighted-on-base-averages">Application: Weighted on-base averages<a class="anchor" aria-label="anchor" href="#application-weighted-on-base-averages"></a>
</h2>
<p>A longstanding tradition in empirical Bayes research is to include an
analysis of batting averages using data from Major League Baseball <span class="citation">(see, for example, Brown 2008; Jiang and Zhang 2010;
and Gu and Koenker 2017)</span>. Until recently, batting averages were
the most important measurement of a hitter’s performance, with the
prestigious yearly “batting title” going to the hitter with the highest
average. However, with the rise of baseball analytics, metrics that
better correlate to teams’ overall run production have become
increasingly preferred. One such metric is wOBA (“weighted on-base
average”), which is both an excellent measure of a hitter’s offensive
production and, unlike competing metrics such as MLB’s xwOBA <span class="citation">(Sharpe 2019)</span> or Baseball Prospectus’s DRC+
<span class="citation">(Judge 2019)</span>, can be calculated using
publicly available data and methods.</p>
<p>Initially proposed by <span class="citation">Tango, Lichtman, and
Dolphin (2006)</span>, wOBA assigns values (“weights”) to hitting
outcomes according to how much the outcome contributes on average to run
production. For example, while batting average treats singles
identically to home runs, wOBA gives a hitter more than twice as much
credit for a home run.</p>
<p>Given a vector of wOBA weights <span class="math inline">\(\mathbf{w}\)</span>, hitter <span class="math inline">\(i\)</span>’s wOBA is the weighted average <span class="math display">\[\begin{equation}
x_i := \mathbf{w}^T \mathbf{z}^{(i)} / n_i,
\end{equation}\]</span> where <span class="math inline">\(\mathbf{z}^{(i)} = (z_1^{(i)}, \ldots,
z_7^{(i)})\)</span> tallies outcomes (singles, doubles, triples, home
runs, walks, hit-by-pitches, and outs) over the hitter’s <span class="math inline">\(n_i\)</span> plate appearances (PAs). Modeling
hitting outcomes as i.i.d. <span class="math display">\[\begin{equation}
\mathbf{z}^{(i)} \sim \text{Multinomial}(n_i, {\boldsymbol\pi}^{(i)}),
\end{equation}\]</span> where <span class="math inline">\({\boldsymbol\pi}^{(i)} = (\pi_1^{(i)}, \ldots,
\pi_7^{(i)})\)</span> is the vector of “true” outcome probabilities for
hitter <span class="math inline">\(i\)</span>, we can regard <span class="math inline">\(x_i\)</span> as a point estimate for the hitter’s
“true wOBA skill” <span class="math display">\[\begin{equation}
\theta_i := \mathbf{w}^T {\boldsymbol\pi}^{(i)}.
\end{equation}\]</span> Standard errors for the <span class="math inline">\(x_i\)</span>’s can be estimated as <span class="math display">\[\begin{equation}
s_i^2 = \mathbf{w}^T \hat{\mathbf\Sigma}^{(i)} \mathbf{w}/n_i,
\end{equation}\]</span> where <span class="math inline">\(\hat{\mathbf\Sigma}^{(i)}\)</span> is the estimate
of the multinomial covariance matrix obtained by setting <span class="math inline">\({\boldsymbol\pi} = \hat{\boldsymbol\pi}\)</span>,
where <span class="math display">\[\begin{equation}
\hat{\boldsymbol\pi}^{(i)} = \mathbf{z}^{(i)}/n_i.
\end{equation}\]</span> (To deal with small sample sizes, we
conservatively lower bound each standard error by the standard error
that would be obtained by plugging in league-average event probabilities
<span class="math inline">\(\hat{\boldsymbol\pi}_{\mathrm{lg}} =
\sum_{i=1}^N \mathbf{z}^{(i)}/ \sum_{i=1}^N n_i\)</span>, where <span class="math inline">\(N\)</span> is the number of hitters in the
dataset.)</p>
<p>The relative complexity of wOBA makes it well suited for analysis via
<code>ebnm</code>. With batting average, a common approach is to obtain
empirical Bayes estimates using a beta-binomial model (see, for example,
<span class="citation">Robinson (2017)</span>). With wOBA, one can
estimate hitting outcome probabilities by way of a Dirichlet-multinomial
model; alternatively, one can approximate the likelihood as normal and
fit an EBNM model directly to the observed wOBAs. In the following, we
take the latter approach.</p>
</div>
<div class="section level2">
<h2 id="the-woba-data-set">The <code>wOBA</code> data set<a class="anchor" aria-label="anchor" href="#the-woba-data-set"></a>
</h2>
<p>We begin by loading and inspecting the <code>wOBA</code> data set,
which consists of wOBAs and standard errors for the 2022 MLB regular
season:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://stephenslab.github.io/ebnm/" class="external-link">ebnm</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="va">wOBA</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html" class="external-link">nrow</a></span><span class="op">(</span><span class="va">wOBA</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 688</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">wOBA</span><span class="op">)</span></span>
<span><span class="co">#&gt;   FanGraphsID           Name Team  PA     x     s</span></span>
<span><span class="co">#&gt; 1       19952     Khalil Lee  NYM   2 1.036 0.733</span></span>
<span><span class="co">#&gt; 2       16953 Chadwick Tromp  ATL   4 0.852 0.258</span></span>
<span><span class="co">#&gt; 3       19608     Otto Lopez  TOR  10 0.599 0.162</span></span>
<span><span class="co">#&gt; 4       24770   James Outman  LAD  16 0.584 0.151</span></span>
<span><span class="co">#&gt; 5        8090 Matt Carpenter  NYY 154 0.472 0.054</span></span>
<span><span class="co">#&gt; 6       15640    Aaron Judge  NYY 696 0.458 0.024</span></span></code></pre></div>
<p>Column “x” contains each player’s wOBA for the 2022 season, which we
regard as an estimate of the player’s “true” wOBA skill. Column “s”
provides standard errors.</p>
<p>Next, we visualize the overall distribution of wOBAs:</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org" class="external-link">ggplot2</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html" class="external-link">ggplot</a></span><span class="op">(</span><span class="va">wOBA</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html" class="external-link">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_histogram.html" class="external-link">geom_histogram</a></span><span class="op">(</span>bins <span class="op">=</span> <span class="fl">64</span>, color <span class="op">=</span> <span class="st">"white"</span>,fill <span class="op">=</span> <span class="st">"black"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html" class="external-link">theme_classic</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<p><img src="ebnm_files/figure-html/unnamed-chunk-2-1.png" width="384"></p>
<p>As the histogram shows, most players finished the season with a wOBA
between .200 and .400. A few had very high wOBAs (<span class="math inline">\(&gt;\)</span>.500), while others had wOBAs at or
near zero. A casual inspection of the data suggests that players with
these very high (or very low) wOBAs were simply lucky (or unlucky). For
example, the 4 players with the highest wOBAs each had 16 PAs or fewer.
It is very unlikely that they would have sustained this high level of
production over a full season’s worth of PAs.</p>
<p>In contrast, Aaron Judge’s production — which included a
record-breaking number of home runs — appears to be “real,” since it was
sustained over nearly 700 PAs. Other cases are more ambiguous: how, for
example, are we to assess Matt Carpenter, who had several exceptional
seasons between 2013 and 2018 but whose output steeply declined in
2019–2021 before his surprising “comeback” in 2022? An empirical Bayes
analysis can help to answer this and other questions.</p>
</div>
<div class="section level2">
<h2 id="main-ebnm-methods-and-the-normal-prior-family">Main <code>ebnm</code> methods and the normal prior family<a class="anchor" aria-label="anchor" href="#main-ebnm-methods-and-the-normal-prior-family"></a>
</h2>
<p>Function <code><a href="../reference/ebnm.html">ebnm()</a></code> is the main interface for fitting the
empirical Bayes normal means model; it is a “Swiss army knife” that
allows for various choices of prior family <span class="math inline">\(\mathcal{G}\)</span> as well as providing multiple
options for fitting and tuning models. For example, we can fit a normal
means model with the prior family <span class="math inline">\(\mathcal{G}\)</span> taken to be the family of
normal distributions:</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">x</span> <span class="op">&lt;-</span> <span class="va">wOBA</span><span class="op">$</span><span class="va">x</span></span>
<span><span class="va">s</span> <span class="op">&lt;-</span> <span class="va">wOBA</span><span class="op">$</span><span class="va">s</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/names.html" class="external-link">names</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="va">wOBA</span><span class="op">$</span><span class="va">Name</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/names.html" class="external-link">names</a></span><span class="op">(</span><span class="va">s</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="va">wOBA</span><span class="op">$</span><span class="va">Name</span></span>
<span><span class="va">fit_normal</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/ebnm.html">ebnm</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">s</span>, prior_family <span class="op">=</span> <span class="st">"normal"</span>, mode <span class="op">=</span> <span class="st">"estimate"</span><span class="op">)</span></span></code></pre></div>
<p>(The default behavior is to fix the prior mode at zero. Since we
certainly do not expect the distribution of true wOBA skill to have a
mode at zero, we set <code>mode = "estimate"</code>.)</p>
<p>We note in passing that the <code>ebnm</code> package has a second
model-fitting interface, in which each prior family gets its own
function:</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">fit_normal</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/ebnm_normal.html">ebnm_normal</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">s</span>, mode <span class="op">=</span> <span class="st">"estimate"</span><span class="op">)</span></span></code></pre></div>
<p>Textual and graphical overviews of results can be obtained using,
respectively, methods <code><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary()</a></code> and <code><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot()</a></code>.
The summary method appears as follows:</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">fit_normal</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; ebnm_normal(x = x, s = s, mode = "estimate")</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; EBNM model was fitted to 688 observations with _heteroskedastic_ standard errors.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; The fitted prior belongs to the _normal_ prior family.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; 2 degrees of freedom were used to estimate the model.</span></span>
<span><span class="co">#&gt; The log likelihood is 989.64.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Available posterior summaries: _mean_, _sd_.</span></span>
<span><span class="co">#&gt; Use method fitted() to access available summaries.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; A posterior sampler is _not_ available.</span></span>
<span><span class="co">#&gt; One can be added via function ebnm_add_sampler().</span></span></code></pre></div>
<p>The <code><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot()</a></code> method visualizes results, comparing the
“observed” values <span class="math inline">\(x_i\)</span> (the initial
wOBA estimates) against the empirical Bayes posterior mean estimates
<span class="math inline">\(\hat{\theta}_i\)</span>:</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">fit_normal</span><span class="op">)</span></span></code></pre></div>
<p><img src="ebnm_files/figure-html/unnamed-chunk-6-1.png" width="336"></p>
<p>The dashed line shows the diagonal <span class="math inline">\(x =
y\)</span>, which makes shrinkage effects clearly visible. In
particular, the most extreme wOBAs on either end of the spectrum are
strongly shrunk towards the league average (around .300).</p>
<p>Since <code><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot()</a></code> returns a “ggplot” object <span class="citation">(Wickham 2016)</span>, the plot can conveniently be
customized using <code>ggplot2</code> syntax. For example, one can vary
the color of the points by the number of plate appearances:</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">fit_normal</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html" class="external-link">geom_point</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html" class="external-link">aes</a></span><span class="op">(</span>color <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">sqrt</a></span><span class="op">(</span><span class="va">wOBA</span><span class="op">$</span><span class="va">PA</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html" class="external-link">labs</a></span><span class="op">(</span>x <span class="op">=</span> <span class="st">"wOBA"</span>, y <span class="op">=</span> <span class="st">"EB estimate of true wOBA skill"</span>, </span>
<span>       color <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html" class="external-link">expression</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">sqrt</a></span><span class="op">(</span><span class="va">PA</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/scale_gradient.html" class="external-link">scale_color_gradient</a></span><span class="op">(</span>low <span class="op">=</span> <span class="st">"blue"</span>, high <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span></span></code></pre></div>
<p><img src="ebnm_files/figure-html/unnamed-chunk-7-1.png" width="384"></p>
<p>By varying the color of points, we see that the wOBA estimates with
higher standard errors or fewer plate appearances (blue points) tend to
be shrunk toward the league average much more strongly than wOBAs from
hitters with many plate appearances (red points).</p>
<p>Above, we used <code><a href="https://rdrr.io/r/utils/head.html" class="external-link">head()</a></code> to view data for the first 6
hitters in the dataset. Let’s now see what the EBNM analysis suggests
might be their “true” wOBA skill. To examine the results more closely,
we use the <code><a href="https://rdrr.io/r/stats/fitted.values.html" class="external-link">fitted()</a></code> method, which returns a posterior
summary for each hitter:</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/fitted.values.html" class="external-link">fitted</a></span><span class="op">(</span><span class="va">fit_normal</span><span class="op">)</span><span class="op">)</span>, digits <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt;                 mean     sd</span></span>
<span><span class="co">#&gt; Khalil Lee     0.303 0.0287</span></span>
<span><span class="co">#&gt; Chadwick Tromp 0.308 0.0286</span></span>
<span><span class="co">#&gt; Otto Lopez     0.310 0.0283</span></span>
<span><span class="co">#&gt; James Outman   0.311 0.0282</span></span>
<span><span class="co">#&gt; Matt Carpenter 0.339 0.0254</span></span>
<span><span class="co">#&gt; Aaron Judge    0.394 0.0184</span></span></code></pre></div>
<p>The wOBA estimates of the first four ballplayers are shrunk strongly
toward the league average, reflecting the fact that these players had
very few plate appearances (and indeed, we were not swayed by their very
high initial wOBA estimates).</p>
<p>Carpenter had many more plate appearances (154) than these other four
players, but according to this model we should remain skeptical about
his strong performance; after factoring in the prior, we judge his
“true” performance to be much closer to the league average, downgrading
an initial estimate of .472 to the final posterior mean estimate of
.339.</p>
</div>
<div class="section level2">
<h2 id="comparing-normal-and-unimodal-prior-families">Comparing normal and unimodal prior families<a class="anchor" aria-label="anchor" href="#comparing-normal-and-unimodal-prior-families"></a>
</h2>
<p>Judge’s “true” wOBA is also estimated to be much lower (.394) than
the initial estimate (.458) despite sustaining a high level of
production over a full season (696 PAs). For this reason, one might ask
whether a prior that is more flexible than the normal prior—that is, a
prior that can better adapt to “outliers” like Judge—might produce a
different result. The <code>ebnm</code> package is very well suited to
answering this question. For example, to obtain results using the family
of all unimodal distributions rather than the family of normal
distributions, we only need to change <code>prior_family</code> from
<code>"normal"</code> to <code>"unimodal"</code>:</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">fit_unimodal</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/ebnm.html">ebnm</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">s</span>, prior_family <span class="op">=</span> <span class="st">"unimodal"</span>, mode <span class="op">=</span> <span class="st">"estimate"</span><span class="op">)</span></span></code></pre></div>
<p>It is straightforward to produce a side-by-side visualization of the
fitted models simply by including both models as arguments to the
<code><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot()</a></code> method (we also use the <code>subset</code> argument
to focus on the results for Judge and other players with the most plate
appearances):</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">top50</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/order.html" class="external-link">order</a></span><span class="op">(</span><span class="va">wOBA</span><span class="op">$</span><span class="va">PA</span>, decreasing <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="va">top50</span> <span class="op">&lt;-</span> <span class="va">top50</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">50</span><span class="op">]</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">fit_normal</span>, <span class="va">fit_unimodal</span>, subset <span class="op">=</span> <span class="va">top50</span><span class="op">)</span></span></code></pre></div>
<p><img src="ebnm_files/figure-html/unnamed-chunk-10-1.png" width="504"></p>
<p>This plot illustrates the ability of the unimodal prior to better
adapt to the data: wOBA estimates for players with a lot of plate
appearances are not adjusted quite so strongly toward the league
average. To compare in more detail, we see for example that Judge’s wOBA
estimate from the model with the unimodal prior (the
<code>"mean2"</code> column) remains much closer to the original wOBA
estimate:</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">dat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html" class="external-link">cbind</a></span><span class="op">(</span><span class="va">wOBA</span><span class="op">[</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"PA"</span>,<span class="st">"x"</span><span class="op">)</span><span class="op">]</span>,</span>
<span>             <span class="fu"><a href="https://rdrr.io/r/stats/fitted.values.html" class="external-link">fitted</a></span><span class="op">(</span><span class="va">fit_normal</span><span class="op">)</span>,</span>
<span>             <span class="fu"><a href="https://rdrr.io/r/stats/fitted.values.html" class="external-link">fitted</a></span><span class="op">(</span><span class="va">fit_unimodal</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/names.html" class="external-link">names</a></span><span class="op">(</span><span class="va">dat</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"PA"</span>, <span class="st">"x"</span>, <span class="st">"mean1"</span>, <span class="st">"sd1"</span>, <span class="st">"mean2"</span>, <span class="st">"sd2"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">dat</span><span class="op">)</span>, digits <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt;                 PA     x mean1    sd1 mean2    sd2</span></span>
<span><span class="co">#&gt; Khalil Lee       2 1.036 0.303 0.0287 0.302 0.0277</span></span>
<span><span class="co">#&gt; Chadwick Tromp   4 0.852 0.308 0.0286 0.307 0.0306</span></span>
<span><span class="co">#&gt; Otto Lopez      10 0.599 0.310 0.0283 0.310 0.0315</span></span>
<span><span class="co">#&gt; James Outman    16 0.584 0.311 0.0282 0.311 0.0318</span></span>
<span><span class="co">#&gt; Matt Carpenter 154 0.472 0.339 0.0254 0.355 0.0430</span></span>
<span><span class="co">#&gt; Aaron Judge    696 0.458 0.394 0.0184 0.439 0.0155</span></span></code></pre></div>
<p>Carpenter’s wOBA estimate is also higher under the more flexible
unimodal prior, but is still adjusted much more than Judge’s in light of
Carpenter’s smaller sample size. It is also interesting that the
unimodal prior assigns greater uncertainty (the <code>"sd2"</code>
column) to this estimate compared to the normal prior.</p>
<p>Recall that the two normal means models differ only in the priors
used, so we can understand the differences in the shrinkage behavior of
these models by inspecting the priors. Calling <code><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot()</a></code> with
<code>incl_cdf = TRUE</code> shows the cumulative distribution functions
(CDFs) of the fitted priors <span class="math inline">\(\hat{g}\)</span>. Since we are particularly
interested in understanding the differences in shrinkage behaviour for
the largest wOBAs such as Judge’s, we create a second plot that zooms in
on wOBAs over .350:</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://wilkelab.org/cowplot/" class="external-link">cowplot</a></span><span class="op">)</span></span>
<span><span class="va">p1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">fit_normal</span>, <span class="va">fit_unimodal</span>, incl_cdf <span class="op">=</span> <span class="cn">TRUE</span>, incl_pm <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/lims.html" class="external-link">xlim</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">.250</span>, <span class="fl">.350</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/guides.html" class="external-link">guides</a></span><span class="op">(</span>color <span class="op">=</span> <span class="st">"none"</span><span class="op">)</span></span>
<span><span class="va">p2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">fit_normal</span>, <span class="va">fit_unimodal</span>, incl_cdf <span class="op">=</span> <span class="cn">TRUE</span>, incl_pm <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/lims.html" class="external-link">lims</a></span><span class="op">(</span>x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">.350</span>, <span class="fl">.450</span><span class="op">)</span>, y <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0.95</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://wilkelab.org/cowplot/reference/plot_grid.html" class="external-link">plot_grid</a></span><span class="op">(</span><span class="va">p1</span>, <span class="va">p2</span>, nrow <span class="op">=</span> <span class="fl">1</span>, ncol <span class="op">=</span> <span class="fl">2</span>, rel_widths <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">3</span>,<span class="fl">5</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p><img src="ebnm_files/figure-html/unnamed-chunk-12-1.png" width="768"></p>
<p>The plot on the right shows that the fitted normal prior has almost
no mass on wOBAs above .400, explaining why Judge’s wOBA estimate is
shrunk so strongly toward the league average, whereas the unimodal prior
is flexible enough to permit larger posterior estimates above .400.</p>
<p>The posterior means and standard errors returned from the
<code><a href="../reference/ebnm.html">ebnm()</a></code> call cannot be used to obtain credible intervals
(except for the special case of the normal prior). Therefore, we provide
additional methods <code><a href="https://rdrr.io/r/stats/confint.html" class="external-link">confint()</a></code> and <code><a href="https://rdrr.io/r/stats/quantile.html" class="external-link">quantile()</a></code>
which return, respectively, credible intervals (or more precisely,
<em>highest posterior density</em> intervals: <span class="citation">E.
P. Box and Tiao (1965)</span>, <span class="citation">Chen and Shao
(1999)</span>) and posterior quantiles for each observation. These are
implemented using Monte Carlo techniques, which can be slow for large
data sets, so credible intervals are not computed by default. The
following code computes 80% highest posterior density (HPD) intervals
for the EBNM model with unimodal prior. (We add a Monte Carlo sampler
using function <code><a href="../reference/ebnm_add_sampler.html">ebnm_add_sampler()</a></code>; alternatively, we could
have added a sampler in our initial calls to <code><a href="../reference/ebnm.html">ebnm()</a></code> by
specifying <code>output = output_all()</code>.) We set a seed for
reproducibility:</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">fit_unimodal</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/ebnm_add_sampler.html">ebnm_add_sampler</a></span><span class="op">(</span><span class="va">fit_unimodal</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/confint.html" class="external-link">confint</a></span><span class="op">(</span><span class="va">fit_unimodal</span>, level <span class="op">=</span> <span class="fl">0.8</span><span class="op">)</span><span class="op">)</span>, digits <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt;                CI.lower CI.upper</span></span>
<span><span class="co">#&gt; Khalil Lee        0.277    0.328</span></span>
<span><span class="co">#&gt; Chadwick Tromp    0.277    0.334</span></span>
<span><span class="co">#&gt; Otto Lopez        0.277    0.336</span></span>
<span><span class="co">#&gt; James Outman      0.277    0.335</span></span>
<span><span class="co">#&gt; Matt Carpenter    0.277    0.389</span></span>
<span><span class="co">#&gt; Aaron Judge       0.428    0.458</span></span></code></pre></div>
<p>Interestingly, the 80% credible interval for Carpenter is very wide,
and shares the same lower bound as the first four ballplayers with very
few plate appearances.</p>
</div>
<div class="section level2">
<h2 id="nonparametric-prior-families-and-advanced-usage">Nonparametric prior families and advanced usage<a class="anchor" aria-label="anchor" href="#nonparametric-prior-families-and-advanced-usage"></a>
</h2>
<p>Above, we demonstrated how the <code>ebnm</code> package makes it is
easy to perform EBNM analyses with different types of priors, then
compared results across two different choices of prior family. Each of
these families makes different assumptions about the data which, <em>a
priori</em>, may be more or less plausible. An alternative to prior
families that make specific assumptions about the data is to use the
prior family that contains <em>all</em> distributions <span class="math inline">\(\mathcal{G}_{\mathrm{npmle}}\)</span>, which is in
a sense “assumption free.” Here we re-analyze the wOBA data set to
illustrate the use of this prior family. Note that although
nonparametric priors require specialized computational techniques,
switching to a nonparametric prior is seamless in <code>ebnm</code>, as
these implementation details are hidden. Similar to above, we need only
make a single change to the <code>prior_family</code> argument:</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">fit_npmle</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/ebnm.html">ebnm</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">s</span>, prior_family <span class="op">=</span> <span class="st">"npmle"</span><span class="op">)</span></span></code></pre></div>
<p>(Note that because the family <span class="math inline">\(\mathcal{G}_{\mathrm{npmle}}\)</span> is not
unimodal, the <code>mode = "estimate"</code> option is not relevant
here.)</p>
<p>Although the implementation details are hidden by default, it can
sometimes be helpful to see what is going on “behind the scenes,”
particularly for flagging or diagnosing issues. By default,
<code>ebnm</code> uses the <code>mixsqp</code> package <span class="citation">(Kim et al. 2020)</span> to fit the NPMLE <span class="math inline">\(\hat{g} \in \mathcal{G}_{\mathrm{npmle}}\)</span>.
We can monitor convergence of the mix-SQP optimization algorithm by
setting the <code>verbose</code> control argument to
<code>TRUE</code>:</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">fit_npmle</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/ebnm.html">ebnm</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">s</span>, prior_family <span class="op">=</span> <span class="st">"npmle"</span>, </span>
<span>                  control <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>verbose <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; Running mix-SQP algorithm 0.3-48 on 688 x 95 matrix</span></span>
<span><span class="co">#&gt; convergence tol. (SQP):     1.0e-08</span></span>
<span><span class="co">#&gt; conv. tol. (active-set):    1.0e-10</span></span>
<span><span class="co">#&gt; zero threshold (solution):  1.0e-08</span></span>
<span><span class="co">#&gt; zero thresh. (search dir.): 1.0e-14</span></span>
<span><span class="co">#&gt; l.s. sufficient decrease:   1.0e-02</span></span>
<span><span class="co">#&gt; step size reduction factor: 7.5e-01</span></span>
<span><span class="co">#&gt; minimum step size:          1.0e-08</span></span>
<span><span class="co">#&gt; max. iter (SQP):            1000</span></span>
<span><span class="co">#&gt; max. iter (active-set):     20</span></span>
<span><span class="co">#&gt; number of EM iterations:    10</span></span>
<span><span class="co">#&gt; Computing SVD of 688 x 95 matrix.</span></span>
<span><span class="co">#&gt; Matrix is not low-rank; falling back to full matrix.</span></span>
<span><span class="co">#&gt; iter        objective max(rdual) nnz stepsize max.diff nqp nls</span></span>
<span><span class="co">#&gt;    1 +9.583407733e-01  -- EM --   95 1.00e+00 6.08e-02  --  --</span></span>
<span><span class="co">#&gt;    2 +8.298700300e-01  -- EM --   95 1.00e+00 2.87e-02  --  --</span></span>
<span><span class="co">#&gt;    3 +7.955308369e-01  -- EM --   95 1.00e+00 1.60e-02  --  --</span></span>
<span><span class="co">#&gt;    4 +7.819858634e-01  -- EM --   68 1.00e+00 1.05e-02  --  --</span></span>
<span><span class="co">#&gt;    5 +7.753787534e-01  -- EM --   53 1.00e+00 7.57e-03  --  --</span></span>
<span><span class="co">#&gt;    6 +7.717040208e-01  -- EM --   49 1.00e+00 5.73e-03  --  --</span></span>
<span><span class="co">#&gt;    7 +7.694760705e-01  -- EM --   47 1.00e+00 4.48e-03  --  --</span></span>
<span><span class="co">#&gt;    8 +7.680398878e-01  -- EM --   47 1.00e+00 3.58e-03  --  --</span></span>
<span><span class="co">#&gt;    9 +7.670690681e-01  -- EM --   44 1.00e+00 2.91e-03  --  --</span></span>
<span><span class="co">#&gt;   10 +7.663865515e-01  -- EM --   42 1.00e+00 2.40e-03  --  --</span></span>
<span><span class="co">#&gt;    1 +7.658902386e-01 +6.493e-02  39  ------   ------   --  --</span></span>
<span><span class="co">#&gt;    2 +7.655114904e-01 +5.285e-02  19 1.00e+00 9.88e-02  20   1</span></span>
<span><span class="co">#&gt;    3 +7.627839841e-01 +1.411e-02   7 1.00e+00 1.28e-01  20   1</span></span>
<span><span class="co">#&gt;    4 +7.626270875e-01 +2.494e-04   7 1.00e+00 3.23e-01   8   1</span></span>
<span><span class="co">#&gt;    5 +7.626270755e-01 +1.748e-08   7 1.00e+00 4.94e-04   2   1</span></span>
<span><span class="co">#&gt;    6 +7.626270755e-01 -2.796e-08   7 1.00e+00 2.76e-07   2   1</span></span>
<span><span class="co">#&gt; Optimization took 0.03 seconds.</span></span>
<span><span class="co">#&gt; Convergence criteria met---optimal solution found.</span></span></code></pre></div>
<p>This output shows no issues with convergence of the optimization
algorithm; the mix-SQP algorithm converged to the solution (up to
numerical rounding error) in only six iterations. In some cases,
convergence issues can arise when fitting nonparametric models to large
or complex data sets, and revealing the details of the optimization can
help to pinpoint these issues.</p>
<p>Next, we visually compare the three fits obtained so far:</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">fit_normal</span>, <span class="va">fit_unimodal</span>, <span class="va">fit_npmle</span>, incl_cdf <span class="op">=</span> <span class="cn">TRUE</span>, subset <span class="op">=</span> <span class="va">top50</span><span class="op">)</span></span></code></pre></div>
<p><img src="ebnm_files/figure-html/unnamed-chunk-16-1.png" width="480"><img src="ebnm_files/figure-html/unnamed-chunk-16-2.png" width="480"></p>
<p>As before, estimates largely agree, differing primarily at the tails.
Both the unimodal prior family and the NPMLE are sufficiently flexible
to avoid the strong shrinkage behavior of the normal prior family.</p>
<p>Fits can be compared quantitatively using the <code><a href="https://rdrr.io/r/stats/logLik.html" class="external-link">logLik()</a></code>
method, which, in addition to the log likelihood for each model,
usefully reports the number of free parameters (i.e., degrees of
freedom):</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/logLik.html" class="external-link">logLik</a></span><span class="op">(</span><span class="va">fit_unimodal</span><span class="op">)</span></span>
<span><span class="co">#&gt; 'log Lik.' 992.6578 (df=40)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/logLik.html" class="external-link">logLik</a></span><span class="op">(</span><span class="va">fit_npmle</span><span class="op">)</span></span>
<span><span class="co">#&gt; 'log Lik.' 994.193 (df=94)</span></span></code></pre></div>
<p>A nonparametric prior <span class="math inline">\(\mathcal{G}\)</span> is approximated by <span class="math inline">\(K\)</span> mixture components on a fixed grid,
with the mixture proportions to be estimated. We can infer from the
above output that <span class="math inline">\(\mathcal{G}_\text{npmle}\)</span> has been
approximated as a family of mixtures over a grid of <span class="math inline">\(K = 95\)</span> point masses spanning the range of
the data. (The number of degrees of freedom is one fewer than <span class="math inline">\(K\)</span> because the mixture proportions must
always sum to 1, which removes one degree of freedom from the estimation
of <span class="math inline">\({\boldsymbol\pi}\)</span>.)</p>
<p>The default behaviour for nonparametric prior families is to choose
<span class="math inline">\(K\)</span> such that the likelihood obtained
using estimate <span class="math inline">\(\hat{g}\)</span> should be
(on average) within one log-likelihood unit of the optimal estimate from
among the entire nonparametric family <span class="math inline">\(\mathcal{G}\)</span> <span class="citation">(see
Willwerscheid 2021)</span>. Thus, a finer approximating grid should not
yield a large improvement in the log-likelihood. We can check this by
using <code><a href="../reference/ebnm_scale_npmle.html">ebnm_scale_npmle()</a></code> to create a finer grid:</p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">scale_npmle</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/ebnm_scale_npmle.html">ebnm_scale_npmle</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">s</span>, KLdiv_target <span class="op">=</span> <span class="fl">0.001</span><span class="op">/</span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>, </span>
<span>                                max_K <span class="op">=</span> <span class="fl">1000</span><span class="op">)</span></span>
<span><span class="va">fit_npmle_finer</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/ebnm_npmle.html">ebnm_npmle</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">s</span>, scale <span class="op">=</span> <span class="va">scale_npmle</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/logLik.html" class="external-link">logLik</a></span><span class="op">(</span><span class="va">fit_npmle</span><span class="op">)</span></span>
<span><span class="co">#&gt; 'log Lik.' 994.193 (df=94)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/logLik.html" class="external-link">logLik</a></span><span class="op">(</span><span class="va">fit_npmle_finer</span><span class="op">)</span></span>
<span><span class="co">#&gt; 'log Lik.' 994.2502 (df=528)</span></span></code></pre></div>
<p>As the theory predicts, a much finer grid, with <span class="math inline">\(K = 529\)</span>, results in only a modest
improvement in the log-likelihood. <code>ebnm</code> provides similar
functions to customize grids for unimodal and normal scale mixture prior
families.</p>
<p>One potential issue with the NPMLE is that, since it is discrete (as
the above CDF plot makes apparent), observations are variously shrunk
towards one of the support points, which can result in poor interval
estimates. For illustration, we calculate 10% and 90% quantiles:</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">fit_npmle</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/ebnm_add_sampler.html">ebnm_add_sampler</a></span><span class="op">(</span><span class="va">fit_npmle</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/quantile.html" class="external-link">quantile</a></span><span class="op">(</span><span class="va">fit_npmle</span>, probs <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0.1</span>, <span class="fl">0.9</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>, digits <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt;                  10%   90%</span></span>
<span><span class="co">#&gt; Khalil Lee     0.276 0.342</span></span>
<span><span class="co">#&gt; Chadwick Tromp 0.276 0.342</span></span>
<span><span class="co">#&gt; Otto Lopez     0.276 0.342</span></span>
<span><span class="co">#&gt; James Outman   0.276 0.342</span></span>
<span><span class="co">#&gt; Matt Carpenter 0.309 0.430</span></span>
<span><span class="co">#&gt; Aaron Judge    0.419 0.430</span></span></code></pre></div>
<p>Each credible interval bound is constrained to lie at one of the
support points of the NPMLE <span class="math inline">\(\hat{g}\)</span>. The interval estimate for Judge
strikes us as far too narrow. Indeed, the NPMLE can sometimes yield
degenerate interval estimates:</p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/confint.html" class="external-link">confint</a></span><span class="op">(</span><span class="va">fit_npmle</span>, level <span class="op">=</span> <span class="fl">0.8</span>, parm <span class="op">=</span> <span class="st">"Aaron Judge"</span><span class="op">)</span></span>
<span><span class="co">#&gt;              CI.lower  CI.upper</span></span>
<span><span class="co">#&gt; Aaron Judge 0.4298298 0.4298298</span></span></code></pre></div>
<p>To address this issue, the <code>deconvolveR</code> package <span class="citation">(Narasimhan and Efron 2020)</span> uses a penalized
likelihood that encourages “smooth” priors <span class="math inline">\(\hat{g}\)</span>; that is, priors <span class="math inline">\(\hat{g}\)</span> for which few of the mixture
proportions are zero:</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">fit_deconv</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/ebnm_deconvolver.html">ebnm_deconvolver</a></span><span class="op">(</span><span class="va">x</span> <span class="op">/</span> <span class="va">s</span>, output <span class="op">=</span> <span class="fu"><a href="../reference/ebnm.html">ebnm_output_all</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span> </span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">fit_deconv</span>, incl_cdf <span class="op">=</span> <span class="cn">TRUE</span>, incl_pm <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span></code></pre></div>
<p><img src="ebnm_files/figure-html/unnamed-chunk-21-1.png" width="288"></p>
<p>Note, however, that the “true” means <span class="math inline">\(\theta\)</span> being estimated are <span class="math inline">\(z\)</span>-scores rather than raw wOBA skill, as
package <code>deconvolveR</code> fits a model to <span class="math inline">\(z\)</span>-scores rather than observations and
associated standard errors. While this is reasonable in many settings,
it does not seem appropriate for the present analysis:</p>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/quantile.html" class="external-link">quantile</a></span><span class="op">(</span><span class="va">fit_deconv</span>, probs <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0.1</span>, <span class="fl">0.9</span><span class="op">)</span><span class="op">)</span> <span class="op">*</span> <span class="va">s</span><span class="op">)</span>, digits <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt;                  10%   90%</span></span>
<span><span class="co">#&gt; Khalil Lee     0.400 2.000</span></span>
<span><span class="co">#&gt; Chadwick Tromp 0.563 1.127</span></span>
<span><span class="co">#&gt; Otto Lopez     0.354 0.796</span></span>
<span><span class="co">#&gt; James Outman   0.412 0.742</span></span>
<span><span class="co">#&gt; Matt Carpenter 0.413 0.531</span></span>
<span><span class="co">#&gt; Aaron Judge    0.406 0.459</span></span></code></pre></div>
<p>These interval estimates do not match our basic intuitions; for
example, a wOBA over .600 has never been sustained over a full
season.</p>
</div>
<div class="section level2">
<h2 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-bhadra2019lasso" class="csl-entry">
Bhadra, Anindya, Jyotishka Datta, Nicholas G. Polson, and Brandon
Willard. 2019. <span>“Lasso Meets Horseshoe: A Survey.”</span>
<em>Statistical Science</em> 34 (3): 405–27.
</div>
<div id="ref-BrownBaseball" class="csl-entry">
Brown, Lawrence D. 2008. <span>“In-Season Prediction of Batting
Averages: A Field Test of Empirical <span>B</span>ayes and
<span>B</span>ayes Methodologies.”</span> <em>Annals of Applied
Statistics</em> 2 (1): 113–52.
</div>
<div id="ref-chen-1999" class="csl-entry">
Chen, Ming-Hui, and Qi-Man Shao. 1999. <span>“<span>Monte</span>
<span>Carlo</span> Estimation of <span>Bayesian</span> Credible and
<span>HPD</span> Intervals.”</span> <em>Journal of Computational and
Graphical Statistics</em> 8 (1): 69–92.
</div>
<div id="ref-hpd" class="csl-entry">
E. P. Box., and George C. Tiao. 1965. <span>“Multiparameter Problems
from a Bayesian Point of View.”</span> <em>Annals of Mathematical
Statistics</em> 36 (5): 1468–82.
</div>
<div id="ref-Efron_Book" class="csl-entry">
Efron, Bradley. 2010. <em>Large-Scale Inference: Empirical
<span>Bayes</span> Methods for Estimation, Testing, and Prediction</em>.
Vol. 1. Institute of Mathematical Statistics Monographs. Cambridge, UK:
Cambridge University Press.
</div>
<div id="ref-efron1972limiting" class="csl-entry">
Efron, Bradley, and Carl Morris. 1972. <span>“Limiting the Risk of
<span>Bayes</span> and Empirical <span>Bayes</span>
Estimators—<span>Part</span> <span>II</span>: The Empirical
<span>Bayes</span> Case.”</span> <em>Journal of the American Statistical
Association</em> 67 (337): 130–39.
</div>
<div id="ref-GuKoenkerBaseball" class="csl-entry">
Gu, Jiaying, and Roger Koenker. 2017. <span>“Empirical
<span>B</span>ayesball Remixed: Empirical <span>B</span>ayes Methods for
Longitudinal Data.”</span> <em>Journal of Applied Econometrics</em> 32
(3): 575–99.
</div>
<div id="ref-JiangZhangBaseball" class="csl-entry">
Jiang, Wenhua, and Cun-Hui Zhang. 2010. <span>“Empirical
<span>B</span>ayes <span>I</span>n-Season Prediction of Baseball Batting
Averages.”</span> In <em>Borrowing Strength: Theory Powering
Applications—a <span>F</span>estschrift for <span>L</span>awrence
<span>D</span>. <span>B</span>rown</em>, 6:263–73. Institute of
Mathematical Statistics Collections. Beachwood, OH: Institute of
Mathematical Statistics.
</div>
<div id="ref-Johnstone" class="csl-entry">
Johnstone, Iain. 2019. <span>“Gaussian Estimation: Sequence and Wavelet
Models.”</span> <a href="https://imjohnstone.su.domains/" class="external-link">https://imjohnstone.su.domains/</a>.
</div>
<div id="ref-drcplus" class="csl-entry">
Judge, Jonathan. 2019. <span>“Entirely Beyond WOWY: A Breakdown of
DRC+.”</span> <em>Baseball Prospectus</em>. <a href="https://www.baseballprospectus.com/news/article/48293/entirely-beyond-wowy-a-breakdown-of-drc/" class="external-link">https://www.baseballprospectus.com/news/article/48293/entirely-beyond-wowy-a-breakdown-of-drc/</a>.
</div>
<div id="ref-MixSQP" class="csl-entry">
Kim, Youngseok, Peter Carbonetto, Matthew Stephens, and Mihai Anitescu.
2020. <span>“A Fast Algorithm for Maximum Likelihood Estimation of
Mixture Proportions Using Sequential Quadratic Programming.”</span>
<em>Journal of Computational and Graphical Statistics</em> 29 (2):
261–73.
</div>
<div id="ref-Morris" class="csl-entry">
Morris, Carl N. 1983. <span>“Parametric Empirical <span>Bayes</span>
Inference: Theory and Applications.”</span> <em>Journal of the American
Statistical Association</em> 78 (381): 47–55.
</div>
<div id="ref-NarasimhanEfron" class="csl-entry">
Narasimhan, Balasubramanian, and Bradley Efron. 2020.
<span>“deconvolveR: A g-Modeling Program for Deconvolution and Empirical
<span>Bayes</span> Estimation.”</span> <em>Journal of Statistical
Software</em> 94 (11): 1–20.
</div>
<div id="ref-Robbins51" class="csl-entry">
Robbins, Herbert. 1951. <span>“Asymptotically Subminimax Solutions of
Compound Statistical Decision Problems.”</span> In <em>Proceedings of
the <span>S</span>econd <span>B</span>erkeley <span>S</span>ymposium on
<span>M</span>athematical <span>S</span>tatistics and
<span>P</span>robability, 1951, Vol. <span>II</span></em>, 131–49.
University of California Press, Berkeley; Los Angeles, CA.
</div>
<div id="ref-Robbins56" class="csl-entry">
———. 1956. <span>“An Empirical <span>B</span>ayes Approach to
Statistics.”</span> In <em>Proceedings of the <span>T</span>hird
<span>B</span>erkeley <span>S</span>ymposium on
<span>M</span>athematical <span>S</span>tatistics and
<span>P</span>robability, 1956, Vol. <span>I</span></em>, 157–63.
University of California Press, Berkeley; Los Angeles, CA.
</div>
<div id="ref-robinson" class="csl-entry">
Robinson, David. 2017. <span>“Introduction to Empirical
<span>Bayes</span>: Examples from Baseball Statistics.”</span> <a href="https://github.com/dgrtwo/empirical-bayes-book" class="external-link">https://github.com/dgrtwo/empirical-bayes-book</a>.
</div>
<div id="ref-xwoba" class="csl-entry">
Sharpe, Sam. 2019. <span>“An Introduction to Expected Weighted
<span>O</span>n-Base Average (xwOBA).”</span> <em>MLB Technology
Blog</em>. <a href="https://technology.mlblogs.com/an-introduction-to-expected-weighted-on-base-average-xwoba-29d6070ba52b" class="external-link">https://technology.mlblogs.com/an-introduction-to-expected-weighted-on-base-average-xwoba-29d6070ba52b</a>.
</div>
<div id="ref-Stephens_NewDeal" class="csl-entry">
Stephens, Matthew. 2017. <span>“False Discovery Rates: A New
Deal.”</span> <em>Biostatistics</em> 18 (2): 275–94.
</div>
<div id="ref-lei-thesis" class="csl-entry">
Sun, Lei. 2020. <span>“Topics on Empirical Bayes Normal Means.”</span>
PhD thesis, Chicago, IL: University of Chicago.
</div>
<div id="ref-Tango" class="csl-entry">
Tango, T. M., M. G. Lichtman, and A. E. Dolphin. 2006. <em>The Book:
Playing the Percentages in Baseball</em>. TMA Press.
</div>
<div id="ref-ggplot2" class="csl-entry">
Wickham, Hadley. 2016. <em><span class="nocase">ggplot2</span>: Elegant
Graphics for Data Analysis</em>. New York, NY: Springer-Verlag.
</div>
<div id="ref-WillwerscheidDiss" class="csl-entry">
Willwerscheid, Jason. 2021. <span>“Empirical <span>Bayes</span> Matrix
Factorization: Methods and Applications.”</span> PhD thesis, Chicago,
IL: University of Chicago.
</div>
</div>
</div>
<div class="section level2">
<h2 id="session-information">Session information<a class="anchor" aria-label="anchor" href="#session-information"></a>
</h2>
<p>The following R version and packages were used to generate this
vignette:</p>
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/sessionInfo.html" class="external-link">sessionInfo</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#&gt; R version 4.2.1 (2022-06-23)</span></span>
<span><span class="co">#&gt; Platform: aarch64-apple-darwin20 (64-bit)</span></span>
<span><span class="co">#&gt; Running under: macOS Monterey 12.3</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Matrix products: default</span></span>
<span><span class="co">#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRblas.0.dylib</span></span>
<span><span class="co">#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.2-arm64/Resources/lib/libRlapack.dylib</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; locale:</span></span>
<span><span class="co">#&gt; [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; attached base packages:</span></span>
<span><span class="co">#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; other attached packages:</span></span>
<span><span class="co">#&gt; [1] cowplot_1.1.1 ggplot2_3.4.3 ebnm_1.1-0   </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; loaded via a namespace (and not attached):</span></span>
<span><span class="co">#&gt;  [1] Rcpp_1.0.11        horseshoe_0.2.0    invgamma_1.1       lattice_0.21-9    </span></span>
<span><span class="co">#&gt;  [5] rprojroot_2.0.3    digest_0.6.33      utf8_1.2.3         truncnorm_1.0-9   </span></span>
<span><span class="co">#&gt;  [9] R6_2.5.1           evaluate_0.22      pillar_1.9.0       rlang_1.1.1       </span></span>
<span><span class="co">#&gt; [13] rstudioapi_0.15.0  irlba_2.3.5.1      jquerylib_0.1.4    Matrix_1.6-1.1    </span></span>
<span><span class="co">#&gt; [17] rmarkdown_2.25     pkgdown_2.0.7      textshaping_0.3.7  desc_1.4.2        </span></span>
<span><span class="co">#&gt; [21] labeling_0.4.3     splines_4.2.1      stringr_1.5.0      munsell_0.5.0     </span></span>
<span><span class="co">#&gt; [25] mixsqp_0.3-48      compiler_4.2.1     xfun_0.40          pkgconfig_2.0.3   </span></span>
<span><span class="co">#&gt; [29] systemfonts_1.0.5  SQUAREM_2021.1     htmltools_0.5.6.1  tidyselect_1.2.0  </span></span>
<span><span class="co">#&gt; [33] tibble_3.2.1       fansi_1.0.5        dplyr_1.1.3        withr_2.5.1       </span></span>
<span><span class="co">#&gt; [37] grid_4.2.1         jsonlite_1.8.7     gtable_0.3.4       lifecycle_1.0.3   </span></span>
<span><span class="co">#&gt; [41] magrittr_2.0.3     scales_1.2.1       cli_3.6.1          stringi_1.7.12    </span></span>
<span><span class="co">#&gt; [45] cachem_1.0.8       farver_2.1.1       fs_1.6.3           bslib_0.5.1       </span></span>
<span><span class="co">#&gt; [49] ragg_1.2.6         generics_0.1.3     vctrs_0.6.3        trust_0.1-8       </span></span>
<span><span class="co">#&gt; [53] RColorBrewer_1.1-3 tools_4.2.1        glue_1.6.2         purrr_1.0.2       </span></span>
<span><span class="co">#&gt; [57] fastmap_1.1.1      yaml_2.3.7         colorspace_2.1-0   ashr_2.2-63       </span></span>
<span><span class="co">#&gt; [61] memoise_2.0.1      deconvolveR_1.2-1  knitr_1.44         sass_0.4.7</span></span></code></pre></div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by Jason Willwerscheid, Matthew Stephens, Peter Carbonetto.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

      </footer>
</div>

  


  

  </body>
</html>

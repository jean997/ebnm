---
title: "ebnm prior families"
author: "Jason Willwerscheid"
date: "1/8/2021"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Prior Families}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 7, fig.height = 5)
```

## Introduction

Below, I use the following prior families. In each case, the mode can either be fixed  or estimated from the data. By default, the mode is fixed at zero.

* `ebnm_normal`: The family of normal distributions.

* `ebnm_point_normal`: The family of spike-and-slab priors with one component a point mass and the other a normal distribution.

* `ebnm_point_laplace`: One component is a point mass and the other is a double-exponential distribution.

* `ebnm_normal_scale_mixture`: The family of scale mixtures of normals.

* `ebnm_ash(mixcompdist = "normal")`: This is also the family of scale mixtures of normals, but whereas `ebnm_normal_scale_mixture` calls into an `ebnm` function that was designed for simplicity and speed, `ebnm_ash` calls directly into the more flexible `ashr::ash`. 

* `ebnm_unimodal_symmetric`: The family of symmetric unimodal distributions.

* `ebnm_unimodal`: The general family of unimodal distributions.

## Code

The following functions are used to simulate data and plot results.

```{r sampling_fns}
# 'mb.times' sets 'times' for microbenchmark. The default is set to be small 
#   so that the vignette runs quickly, but much better results will be obtained 
#   when it's larger.
mb.times <- 5L 

library(ebnm)
library(microbenchmark)
library(ggplot2)

sample <- function(g, ...) {
  UseMethod("sample", g)
}

sample.normalmix <- function(g, nsamp) {
  which.comp <- rmultinom(nsamp, size = 1, prob = g$pi)
  means <- g$mean %*% which.comp
  sds <- g$sd %*% which.comp
  return(rnorm(nsamp, means, sds))
}

sample.unimix <- function(g, nsamp) {
  which.comp <- rmultinom(nsamp, size = 1, prob = g$pi)
  a <- g$a %*% which.comp
  b <- g$b %*% which.comp
  return(runif(nsamp, a, b))
}

cdf <- function(g, ...) {
  UseMethod("cdf", g)
}

cdf.normalmix <- function(g, x) {
  k <- length(g$pi)
  p <- matrix(pnorm(rep(x, each = k), g$mean, g$sd), nrow = k)
  return(as.vector(g$pi %*% p))
}
  
cdf.unimix <- function(g, x) {
  k <- length(g$pi)
  p <- matrix(punif(rep(x, each = k), g$a, g$b), nrow = k)
  return(as.vector(g$pi %*% p))
}

cdf.laplacemix <- function(g, x) {
  k <- length(g$pi)
  p <- matrix(0.5 * pexp(rep(x - g$mean[1], each = k), 1 / g$scale)
              + 0.5 * pexp(rep(-(x - g$mean[1]), each = k), 1 / g$scale, 
                           lower.tail = FALSE), 
              nrow = k)
  return(as.vector(g$pi %*% p))
}

plot.cdfs <- function(g.list, g.names, xmin, xmax, npts = 100) {
  grid <- seq(xmin, xmax, length.out = npts)
  cdf.list <- lapply(g.list, cdf, x = grid)
  df <- data.frame(x = rep(grid, length(g.list)),
                   cdf = unlist(cdf.list),
                   g = rep(g.names, each = length(grid)))
  ggplot(df, aes(x = x, y = cdf, color = g)) + geom_line()  
}
```

## Example 1: mixture of normals

First I simulate observations from the prior

$$ g \sim 0.6\ \delta_0 + 0.3\ \text{Normal}(0, 3^2) + 0.1\ \text{Normal}(0, 10^2), $$

which is meant to evoke a point-normal distribution with a slightly heavier tail. I plot the cumulative distribution functions for the true and estimated priors below. Since all distributions are symmetric about zero, I only show the left tails.

```{r g1.plot}
true.g <- ashr::normalmix(pi = c(0.6, 0.3, 0.1),
                          mean = c(0, 0, 0),
                          sd = c(0, 3, 10))

set.seed(666)
n <- 2000
theta <- sample(true.g, n)
s <- 1
x <- theta + rnorm(n)

n.res   <- ebnm_normal(x, s)
pn.res  <- ebnm_point_normal(x, s)
pl.res  <- ebnm_point_laplace(x, s)
smn.res <- ebnm_normal_scale_mixture(x, s)

plot.cdfs(list(true.g, n.res$fitted_g, pn.res$fitted_g, pl.res$fitted_g,
               smn.res$fitted_g),
          g.names = c("true.g", "normal", "point.normal", "point.laplace",
                      "scale.mix"),
          xmin = -10, xmax = 0.5)
```

### Timing: homoskedastic errors

With `s = 1`, a timing comparison yields:

```{r g1.timing.homo}
timing.res <- microbenchmark(ebnm_normal(x, s),
                             ebnm_point_normal(x, s),
                             ebnm_point_laplace(x, s),
                             ebnm_normal_scale_mixture(x, s),
                             ebnm_ash(x, s, mixcompdist = "normal"),
                             ebnm_unimodal_symmetric(x, s),
                             ebnm_unimodal(x, s),
                             times = mb.times)
autoplot(timing.res)
```

### Timing: heteroskedastic errors

With heteroskedastic errors simulated from a Gamma distribution with shape and rate parameters equal to 1, the timings are:

```{r g1.timing.hetero}
s <- rgamma(n, 1, 1)
timing.res <- microbenchmark(ebnm_normal(x, s),
                             ebnm_point_normal(x, s),
                             ebnm_point_laplace(x, s),
                             ebnm_normal_scale_mixture(x, s),
                             ebnm_ash(x, s, mixcompdist = "normal"),
                             ebnm_unimodal_symmetric(x, s),
                             ebnm_unimodal(x, s),
                             times = mb.times)
autoplot(timing.res)
```


<!-- ## Example: mixture of uniforms -->

<!-- Next I simulate observations from the more challenging "flat-top" distribution with a point mass at zero and a bit of extra tail: -->

<!-- $$ g \sim 0.5\ \delta_0 + 0.4\ \text{Uniform}(-1, 1) + 0.1\ \text{Uniform}(-10, 10), $$ -->

<!-- I bump `n` up to 20000 to see how it affects the timing comparisons. -->

<!-- ```{r g2.plot} -->
<!-- true.g <- ashr::unimix(pi = c(0.5, 0.4, 0.1), -->
<!--                        a = c(0, -1, -6), -->
<!--                        b = c(0, 1, 6)) -->

<!-- n <- 20000 -->
<!-- theta <- sample.unimix(true.g, n) -->
<!-- s <- 1 -->
<!-- x <- theta + rnorm(n) -->

<!-- pn.res    <- ebnm_point_normal(x, s) -->
<!-- pl.res    <- ebnm_point_laplace(x, s) -->
<!-- smn.res   <- ebnm_normal_scale_mixture(x, s) -->
<!-- symm.res  <- ebnm_unimodal_symmetric(x, s) -->

<!-- plot.cdfs(list(true.g, pn.res$fitted_g, pl.res$fitted_g, smn.res$fitted_g, -->
<!--                symm.res$fitted_g), -->
<!--           c("true.g", "point.normal", "point.laplace", "scale.mix.normal", -->
<!--             "symm.unimodal"), -->
<!--           xmin = -6, xmax = 0.5) -->
<!-- ``` -->

<!-- ### Timing: homoskedastic errors -->

<!-- ```{r g2.timing.homo} -->
<!-- timing.res <- microbenchmark(ebnm_normal(x, s), -->
<!--                              ebnm_point_normal(x, s), -->
<!--                              ebnm_point_laplace(x, s), -->
<!--                              ebnm_normal_scale_mixture(x, s), -->
<!--                              ebnm_ash(x, s, mixcompdist = "normal"), -->
<!--                              ebnm_unimodal_symmetric(x, s), -->
<!--                              ebnm_unimodal(x, s), -->
<!--                              times = mb.times) -->
<!-- autoplot(timing.res) -->
<!-- ``` -->

<!-- ### Timing: heteroskedastic errors -->

<!-- ```{r g2.timing.hetero} -->
<!-- s <- rgamma(n, 1) -->
<!-- timing.res <- microbenchmark(ebnm_normal(x, s), -->
<!--                              ebnm_point_normal(x, s), -->
<!--                              ebnm_point_laplace(x, s), -->
<!--                              ebnm_normal_scale_mixture(x, s), -->
<!--                              ebnm_ash(x, s, mixcompdist = "normal"), -->
<!--                              ebnm_unimodal_symmetric(x, s), -->
<!--                              ebnm_unimodal(x, s), -->
<!--                              times = mb.times) -->
<!-- autoplot(timing.res) -->
<!-- ``` -->


## Example 2: non-zero mode 

Next I simulate observations from the prior

$$ g \sim 0.8\ \delta_\mu + 0.2\ \text{Normal}(\mu, 2^2), $$

where the true mode $\mu = 1$ is to be estimated from the data.

```{r g3.plot}
true.g <- ashr::normalmix(pi = c(0.8, 0.2),
                          mean = c(1, 1),
                          sd = c(0, 2))

n <- 2000
set.seed(666)
theta <- sample(true.g, n)
s <- 1
x <- theta + rnorm(n)

n.res   <- ebnm_normal(x, s, mode = "estimate")
pn.res  <- ebnm_point_normal(x, s, mode = "estimate")
pl.res  <- ebnm_point_laplace(x, s, mode = "estimate")
smn.res <- ebnm_normal_scale_mixture(x, s, mode = "estimate")

plot.cdfs(list(true.g, n.res$fitted_g, pn.res$fitted_g, pl.res$fitted_g,
               smn.res$fitted_g),
          g.names = c("true.g", "normal", "point.normal", "point.laplace",
                      "scale.mix"),
          xmin = -4, xmax = 1.5)
```

### Timing: homoskedastic errors

With `s = 1`, a timing comparison yields:

```{r g3.timing.homo}
timing.res <- microbenchmark(ebnm_normal(x, s, mode = "estimate"),
                             ebnm_point_normal(x, s, mode = "estimate"),
                             ebnm_point_laplace(x, s, mode = "estimate"),
                             ebnm_normal_scale_mixture(x, s, mode = "estimate"),
                             ebnm_ash(x, s, mixcompdist = "normal", mode = "estimate"),
                             ebnm_unimodal_symmetric(x, s, mode = "estimate"),
                             ebnm_unimodal(x, s, mode = "estimate"),
                             times = mb.times)
autoplot(timing.res)
```

### Timing: heteroskedastic errors

With heteroskedastic errors simulated from a Gamma distribution as above, the timings are:

```{r g3.timing.hetero}
s <- rgamma(n, 1, 1)
timing.res <- microbenchmark(ebnm_normal(x, s, mode = "estimate"),
                             ebnm_point_normal(x, s, mode = "estimate"),
                             ebnm_point_laplace(x, s, mode = "estimate"),
                             ebnm_normal_scale_mixture(x, s, mode = "estimate"),
                             ebnm_ash(x, s, mixcompdist = "normal", mode = "estimate"),
                             ebnm_unimodal_symmetric(x, s, mode = "estimate"),
                             ebnm_unimodal(x, s, mode = "estimate"),
                             times = mb.times)
autoplot(timing.res)
```

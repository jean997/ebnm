---
title: Brief introduction to shrinkage and empirical Bayes 
author: Jason Willwerscheid and Peter Carbonetto
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: no
    highlight: textmate
    theme: readable
vignette: >
  %\VignetteIndexEntry{Brief introduction to shrinkage and empirical Bayes }
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r knitr-opts, include=FALSE}
knitr::opts_chunk$set(comment = "#",collapse = TRUE,results = "hold",
                      fig.align = "center",dpi = 90)
```

This is a very brief illustration of the basic ideas behind the
empirical Bayes methods. See for example [John Storey's
text][storey-book] to learn more.

First, to generate a toy data set, we simulate a bunch of "true"
(unknown) means $u_i$ from a mixture of a normal distribution centered
at 2 and a point-mass also at 2:

$$
u_i \sim 0.8\delta_2 + 0.2 N(2,1)
$$

```{r sim-data}
set.seed(1)
n <- 400
u <- 2 + (runif(n) < 0.2) * rnorm(n)
```

Then we simulate the observations $x_i$ as "noisy" estimates of the
true means:

$$
x_i \sim u_i + N(0,1/3)
$$

```{r sim-data-2}
s <- rep(1/3,n)
x <- u + s*rnorm(n)
```

The maximum-likelihood estimates (MLEs) of the true means are simply
$\hat{u}_i = x_i$:

```{r plot-mle, fig.height=3.5, fig.width=3.5}
par(mar = c(4,4,2,2))
lims <- range(c(u,x))
plot(u,x,pch = 4,cex = 0.75,xlim = lims,ylim = lims)
abline(a = 0,b = 1,col = "magenta",lty = "dotted")
```

We can do much better than the maximum-likelihood estimator—and in
fact the theory tells us we are guaranteed to do better—by learning a
prior from all the observations, and then this prior can be used to
"shrink" the estimates toward the shared prior. This is the essential
idea behind empirical Bayes.

Let's illustrate this idea with a simple normal prior with unknown
mean and unknown variance. (Noting that the normal prior is the "wrong"
prior for this data set.) First, we fit the prior,

```{r ebnm-normal}
library(ebnm)
fit_normal <- ebnm(x,s,prior_family = "normal",mode = "estimate")
```

then we estimate the true means by the posterior mean values,
$\hat{u}_i = E[u_i \,|\, x_i]$, which can be extracted with
`coef()`:

```{r plot-ebnm-normal, fig.height=3.5, fig.width=3.5}
y <- coef(fit_normal)
lims <- range(c(u,y))
par(mar = c(4,4,2,2))
plot(u,y,pch = 4,cex = 0.75,xlim = lims,ylim = lims)
abline(a = 0,b = 1,col = "magenta",lty = "dotted")
```

It turns out the shrinked estimates are worse for the larger $u_i$
values because they are shrunk toward the common mean near 2. Still, the
*overall* estimation error—the "mean-squared error" (MSE)—went down:

```{r mse-1}
mse_mle    <- (x - u)^2
mse_shrink <- (y - u)^2
print(round(digits = 4,
            x = c(mle = mean(mse_mle),
                  normal = mean(mse_shrink))))
```

Here's a more detailed comparison of the estimation error:

```{r plot-mse-1, fig.height=3.5, fig.width=3.5}
par(mar = c(4,4,2,2))
plot(mse_mle,mse_shrink,pch = 4,cex = 0.75,xlim = c(0,1.2),ylim = c(0,1.2))
abline(a = 0,b = 1,col = "magenta",lty = "dotted")
```

Indeed, a few of the large estimates got much worse while many other
estimates got a little bit better.

Now, with the right prior—specifically, the "point-normal" prior in
ebnm—the shrinked estimates get even better.

As before, we first fit the prior to the data (the only difference
here is in the "prior_family" argument):

```{r ebnm-pn}
fit_pn <- ebnm(x,s,prior_family = "point_normal",mode = "estimate")
```

Then we extract the posterior mean estimates:

```{r plot-ebnm-pn, fig.height=3.5, fig.width=3.5}
par(mar = c(4,4,2,2))
y <- coef(fit_pn)
lim <- range(c(u,y))
plot(u,y,pch = 4,cex = 0.75,xlim = lim,ylim = lim)
abline(a = 0,b = 1,col = "magenta",lty = "dotted")
```

The added flexibility of the point-normal allowed the accuracy of the
small values to be improved without increasing the error in the large
values, and so the latest shrunk estimates improved the overall error
even more:

```{r plot-mse-2, fig.height=3.5, fig.width=3.5}
mse_shrink_pn <- (y - u)^2
print(round(digits = 4,
            x = c(mle = mean(mse_mle),
                  normal = mean(mse_shrink),
				  point_normal = mean(mse_shrink_pn))))
```

[storey-book]: https://jdstorey.org/fas

---
title: Brief introduction to shrinkage and empirical Bayes normal means
author: Jason Willwerscheid and Peter Carbonetto
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    toc: yes
bibliography: ebnm.bib
vignette: >
  %\VignetteIndexEntry{Brief introduction to shrinkage and empirical Bayes normal means}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r knitr-opts, include=FALSE}
knitr::opts_chunk$set(comment = "#",collapse = TRUE,results = "hold",
                      fig.align = "center",dpi = 90)
```

## The normal means model and empirical Bayes

Given $n$ observations $x_i$ with known standard deviations $s_i$, $i
= 1, \dots, n$, the normal means model
[@Robbins51; @efron1972limiting; @Stephens_NewDeal; @bhadra2019lasso;
@Johnstone; @lei-thesis] is
\begin{equation}
x_i \overset{\text{ind.}}{\sim} \mathcal{N}(\theta_i, s_i^2),
\end{equation}
in which the unknown (true) means $\theta_i$ are the quantities
to be estimated. We use
$\mathcal{N}(\mu, \sigma^2)$ to denote the normal distribution with
mean $\mu$ and variance $\sigma^2$.

The maximum-likelihood estimate of $\theta_i$ is, of course, $x_i$.

The empirical Bayes (EB) approach to inferring $\theta_i$ attempts to
improve upon the maximum-likelihood estimate by "borrowing
information" across observations, exploiting the fact that each
observation contains information not only about its respective mean,
but also about how the means are collectively distributed
[@Robbins56; @Morris; @Efron_Book; @Stephens_NewDeal].  Specifically,
the EB approach assumes that
\begin{equation}
\theta_i \overset{\text{ind.}}{\sim} g \in \mathcal{G},
\end{equation}
where $g$ is a distribution to be estimated from the data, typically
chosen from among some family of distributions $\mathcal{G}$ that is
specified in advance. (Note that although $\mathcal{G}$ must be
specified in advance, it can be arbitrarily flexible.)

The empirical Bayes normal means model is fit by first using
all of the observations to estimate $g \in \mathcal{G}$, then using
the estimated distribution $\hat{g}$ to compute posteriors for each
mean $\theta_i$. Commonly, $g$ is estimated via maximum-likelihood.

The ebnm package provides a unified interface for efficiently
estimating priors $g \in \mathcal{G}$ and computing posterior
estimates of the unknown means $\theta_i$, with a wide range of
options for prior families $\mathcal{G}$.

Next we give a brief illustration of these ideas. For background, see
for example [John Storey's book][storey-book]. The
[ebnm paper][ebnm-paper] also has a detailed introduction to
the normal means model and empirical Bayes ideas.

## An illustration

Our example data set is 400 data points simulated from a normal means
model in which the true prior $g$ is a mixture of (a) a normal
distribution centered at 2 and (b) a point-mass also centered
at 2:

$$
u_i \sim 0.8\delta_2 + 0.2 N(2,1)
$$

First, we simulate the "true" means, $u_i$, from this prior,

```{r sim-data}
set.seed(1)
n <- 400
u <- 2 + (runif(n) < 0.2) * rnorm(n)
```

then we simulate the observed means, $x_i$, as "noisy" estimates of the
true means (in this example, the noise is the same for all data points):

$$
x_i \sim N(u_i,s_i), \quad s_i = 1/3,
$$

```{r sim-data-2}
s <- rep(1/3,n)
x <- u + s*rnorm(n)
```

Obviously, we know what the true means are in this example, but here
we'll treat them as quantities we cannot observe.

The maximum-likelihood estimates (MLEs) of the true means are simply
$\hat{u}_i = x_i$:

```{r plot-mle, fig.height=3.5, fig.width=3.5}
par(mar = c(4,4,2,2))
lims <- c(-0.55,5.05)
plot(u,x,pch = 4,cex = 0.75,xlim = lims,ylim = lims,
     xlab = "true value (u)",ylab = "estimate (x)",
     main = "MLE")
abline(a = 0,b = 1,col = "magenta",lty = "dotted")
```

We can do much better than the maximum-likelihood estimator—and in
fact some theory tells us we are guaranteed to do better—by learning a
prior from all the observations, then "shrinking" the estimates toward
this prior.

Let's illustrate this idea with a simple *ormal prior in which the
mean and variance of the normal prior are learned from the
data. (Let's note that the normal prior is the wrong prior for this
data set! Recall we simulated with a mixture of a normal and a
point-mass.)

First, we fit the prior,

```{r ebnm-normal}
library(ebnm)
fit_normal <- ebnm(x,s,prior_family = "normal",mode = "estimate")
```

then we estimate the true means by the posterior mean values,
$\hat{u}_i = E[u_i \,|\, x_i]$, which can be extracted with
`coef()`:

```{r plot-ebnm-normal, fig.height=3.5, fig.width=3.5}
y <- coef(fit_normal)
par(mar = c(4,4,2,2))
plot(u,y,pch = 4,cex = 0.75,xlim = lims,ylim = lims,
     xlab = "true value (u)",ylab = "estimate (x)",
     main = "normal prior")
abline(a = 0,b = 1,col = "magenta",lty = "dotted")
```

These "shrunken" estimates are better for smaller $u_i$ values but
worse for the larger values. Still, the shrunken estimates improve
the *overall estimation error* (the "mean-squared error"):

```{r mse-1}
err_mle           <- (x - u)^2
err_shrink_normal <- (y - u)^2
print(round(digits = 4,
            x = c(mle           = mean(err_mle),
                  shrink_normal = mean(err_shrink_normal))))
```

Here's a more detailed comparison of the estimation error:

```{r plot-mse-1, fig.height=3.5, fig.width=3.5}
par(mar = c(4,4,2,2))
plot(err_mle,err_shrink_normal,pch = 4,cex = 0.75,
     xlim = c(0,1.2),ylim = c(0,1.2))
abline(a = 0,b = 1,col = "magenta",lty = "dotted")
```

Indeed, the error increased in a few of the estimates and decreased in
many of the other estimates, resulting in a lower mean-squared
error (MSE) over the 400 data points.

Let's now see what happens when we use a family of priors that is
better suited to this data set—specifically, the "point-normal"
family. Notice that the only change we have made in our call to
`ebnm()` is in the "prior_family" argument:

```{r ebnm-pn}
fit_pn <- ebnm(x,s,prior_family = "point_normal",mode = "estimate")
```

Now we extract the posterior mean estimates and compare to the true
values:

```{r plot-ebnm-pn, fig.height=3.5, fig.width=3.5}
par(mar = c(4,4,2,2))
y <- coef(fit_pn)
plot(u,y,pch = 4,cex = 0.75,xlim = lims,ylim = lims,
     xlab = "true value (u)",ylab = "estimate (x)",
     main = "point-normal prior")
abline(a = 0,b = 1,col = "magenta",lty = "dotted")
```

The added flexibility of the point-normal allowed the accuracy of the
smaller means to be improved, while the estimates of the larger means
the did not get worse. The result is that the overall error improved
over both the MLE and the posterior estimates with a normal prior:

```{r mse-2}
err_shrink_pn <- (y - u)^2
print(round(digits = 4,
            x = c(mle = mean(err_mle),
                  normal = mean(err_shrink_normal),
				  point_normal = mean(err_shrink_pn))))
```

## Session information

The following R version and packages were used to generate this vignette:

```{r}
sessionInfo()
```

## References

[ebnm-paper]: https://arxiv.org/abs/2110.00152
[storey-book]: https://jdstorey.org/fas

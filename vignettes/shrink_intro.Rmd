---
title: Brief introduction to shrinkage and empirical Bayes normal means
author: Jason Willwerscheid and Peter Carbonetto
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    toc: yes
bibliography: ebnm.bib
vignette: >
  %\VignetteIndexEntry{Brief introduction to shrinkage and empirical Bayes}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r knitr-opts, include=FALSE}
knitr::opts_chunk$set(comment = "#",collapse = TRUE,results = "hold",
                      fig.align = "center",dpi = 90)
```

## The normal means model and empirical Bayes

Given $n$ observations $x_i$ with known standard deviations $s_i$, $i
= 1, \dots, n$, the normal means model
[@Robbins51; @efron1972limiting; @Stephens_NewDeal; @bhadra2019lasso;
@Johnstone; @lei-thesis] is
\begin{equation}
x_i \overset{\text{ind.}}{\sim} \mathcal{N}(\theta_i, s_i^2),
\end{equation}
with the unknown (true) means $\theta_i$ to be estimated. We use
$\mathcal{N}(\mu, \sigma^2)$ to denote the normal distribution with
mean $\mu$ and variance $\sigma^2$. The maximum-likelihood estimate of
$\theta_i$ is, of course, $x_i$.

The empirical Bayes (EB) approach to inferring $\theta_i$ attempts to
improve upon the maximum-likelihood estimate by "borrowing
information" across observations, exploiting the fact that each
observation contains information not only about its respective mean,
but also about how the means are collectively distributed
[@Robbins56; @Morris; @Efron_Book; @Stephens_NewDeal].  Specifically,
the EB approach assumes that
\begin{equation}
\theta_i \overset{\text{ind.}}{\sim} g \in \mathcal{G},
\end{equation}
where $g$ is a distribution to be estimated from the data, typically
chosen from among some family of distributions $\mathcal{G}$ that is
specified in advance. (Note that although $\mathcal{G}$ must be
specified in advance, it can be arbitrarily flexible.)

The empirical Bayes normal means model is therefore fit by first using
all of the observations to estimate $g \in \mathcal{G}$, then using
the estimated distribution $\hat{g}$ to compute posteriors for each
mean $\theta_i$. Commonly, $g$ is estimated via maximum-likelihood.

The ebnm package provides a unified interface for efficiently
estimating priors $g \in \mathcal{G}$ and computing posterior
estimates of the unknown means $\theta_i$, with different choices for
prior families $\mathcal{G}$.

Next we give a brief illustration of these ideas. See for example
[John Storey's book][storey-book] for background on these ideas.  See
also the [ebnm paper][ebnm-paper] for a more detailed introduction to
empirical Bayes normal means and the ebnm package.

## A toy example

First, to generate a toy data set, we simulate a bunch of "true"
(unknown) means $u_i$ from a mixture of a normal distribution centered
at 2 and a point-mass also at 2:

$$
u_i \sim 0.8\delta_2 + 0.2 N(2,1)
$$

```{r sim-data}
set.seed(1)
n <- 400
u <- 2 + (runif(n) < 0.2) * rnorm(n)
```

Then we simulate the observations $x_i$ as "noisy" estimates of the
true means:

$$
x_i \sim u_i + N(0,1/3)
$$

```{r sim-data-2}
s <- rep(1/3,n)
x <- u + s*rnorm(n)
```

The maximum-likelihood estimates (MLEs) of the true means are simply
$\hat{u}_i = x_i$:

```{r plot-mle, fig.height=3.5, fig.width=3.5}
par(mar = c(4,4,2,2))
lims <- range(c(u,x))
plot(u,x,pch = 4,cex = 0.75,xlim = lims,ylim = lims)
abline(a = 0,b = 1,col = "magenta",lty = "dotted")
```

We can do much better than the maximum-likelihood estimator—and in
fact the theory tells us we are guaranteed to do better—by learning a
prior from all the observations, and then this prior can be used to
"shrink" the estimates toward the shared prior. This is the essential
idea behind empirical Bayes.

Let's illustrate this idea with a simple normal prior with unknown
mean and unknown variance. (Noting that the normal prior is the "wrong"
prior for this data set.) First, we fit the prior,

```{r ebnm-normal}
library(ebnm)
fit_normal <- ebnm(x,s,prior_family = "normal",mode = "estimate")
```

then we estimate the true means by the posterior mean values,
$\hat{u}_i = E[u_i \,|\, x_i]$, which can be extracted with
`coef()`:

```{r plot-ebnm-normal, fig.height=3.5, fig.width=3.5}
y <- coef(fit_normal)
lims <- range(c(u,y))
par(mar = c(4,4,2,2))
plot(u,y,pch = 4,cex = 0.75,xlim = lims,ylim = lims)
abline(a = 0,b = 1,col = "magenta",lty = "dotted")
```

It turns out the shrinked estimates are worse for the larger $u_i$
values because they are shrunk toward the common mean near 2. Still, the
*overall* estimation error—the "mean-squared error" (MSE)—went down:

```{r mse-1}
mse_mle    <- (x - u)^2
mse_shrink <- (y - u)^2
print(round(digits = 4,
            x = c(mle = mean(mse_mle),
                  normal = mean(mse_shrink))))
```

Here's a more detailed comparison of the estimation error:

```{r plot-mse-1, fig.height=3.5, fig.width=3.5}
par(mar = c(4,4,2,2))
plot(mse_mle,mse_shrink,pch = 4,cex = 0.75,xlim = c(0,1.2),ylim = c(0,1.2))
abline(a = 0,b = 1,col = "magenta",lty = "dotted")
```

Indeed, a few of the large estimates got much worse while many other
estimates got a little bit better.

Now, with the right prior—specifically, the "point-normal" prior in
ebnm—the shrinked estimates get even better.

As before, we first fit the prior to the data (the only difference
here is in the "prior_family" argument):

```{r ebnm-pn}
fit_pn <- ebnm(x,s,prior_family = "point_normal",mode = "estimate")
```

Then we extract the posterior mean estimates:

```{r plot-ebnm-pn, fig.height=3.5, fig.width=3.5}
par(mar = c(4,4,2,2))
y <- coef(fit_pn)
lim <- range(c(u,y))
plot(u,y,pch = 4,cex = 0.75,xlim = lim,ylim = lim)
abline(a = 0,b = 1,col = "magenta",lty = "dotted")
```

The added flexibility of the point-normal allowed the accuracy of the
small values to be improved without increasing the error in the large
values, and so the latest shrunk estimates improved the overall error
even more:

```{r plot-mse-2, fig.height=3.5, fig.width=3.5}
mse_shrink_pn <- (y - u)^2
print(round(digits = 4,
            x = c(mle = mean(mse_mle),
                  normal = mean(mse_shrink),
				  point_normal = mean(mse_shrink_pn))))
```

## Session information

The following R version and packages were used to generate this vignette:

```{r}
sessionInfo()
```

## References

[ebnm-paper]: https://arxiv.org/abs/2110.00152
[storey-book]: https://jdstorey.org/fas

---
title: Getting started with the ebnm package
author: Jason Willwerscheid
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    toc: yes
bibliography: ebnm.bib
vignette: >
  %\VignetteIndexEntry{Getting started with the ebnm package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, collapse = TRUE, comment = "#>",
                      fig.width = 6, fig.height = 4, warning = FALSE)
```

## The empirical Bayes normal means problem

Given $n$ observations $x_i$ with known standard deviations $s_i$, $i
= 1, \dots, n$, the normal means model
[@Robbins51; @efron1972limiting; @Stephens_NewDeal; @bhadra2019lasso;
@Johnstone; @lei-thesis] is \begin{equation} x_i
\overset{\text{ind.}}{\sim} \mathcal{N}(\theta_i, s_i^2),
\end{equation} with the unknown (true) means $\theta_i$ to be
estimated. Here and throughout, we use $\mathcal{N}(\mu, \sigma^2)$ to
denote the normal distribution with mean $\mu$ and variance
$\sigma^2$. The maximum-likelihood estimate of $\theta_i$ is, of
course, $x_i$. The empirical Bayes (EB) approach to inferring
$\theta_i$ attempts to improve upon the maximum-likelihood estimate by
"borrowing information" across observations, exploiting the fact that
each observation contains information not only about its respective
mean, but also about how the means are collectively distributed
[@Robbins56; @Morris; @Efron_Book; @Stephens_NewDeal].  Specifically,
the EB approach assumes that \begin{equation} \theta_i
\overset{\text{ind.}}{\sim} g \in \mathcal{G}, \end{equation} where
$g$ is a distribution to be estimated from the data, typically chosen
from among some family of distributions $\mathcal{G}$ that is
specified in advance. (Note that although $\mathcal{G}$ must be
specified in advance, it can be arbitrarily flexible.)

The empirical Bayes normal means model is
therefore fit by first using all of the
observations to estimate $g \in \mathcal{G}$, then using the
estimated distribution $\hat{g}$ to compute posteriors for each mean
$\theta_i$. Commonly, $g$ is estimated via maximum-likelihood, in
which case the EB approach consists of the following:

1. Find $\hat{g} := \rm{argmax}_{g \,\in\, \mathcal{G}} L(g)$, where
$L(g)$ denotes the marginal likelihood,
\begin{equation} 
L(g) :=  p({\mathbf x} \mid g, {\mathbf s}) = 
\prod_{i=1}^n \textstyle 
\int p(x_i \mid \theta_i, s_i) \, g(\theta_i) \, d\theta_i,
\end{equation}
and we define ${\mathbf x} := (x_1, \ldots, x_n)$, ${\mathbf s}
:= (s_1, \ldots, s_n)$.

1. Compute posterior distributions 
\begin{equation} 
p(\theta_i \mid x_i, s_i, \hat{g}) \propto 
\hat{g}(\theta_i) \, p(x_i \mid \theta_i, s_i),
\end{equation}
and/or summaries (posterior means, variances, etc.).

We refer to this two-step process as "solving the EBNM problem." 
The `ebnm` package provides a unified interface for efficiently solving
the EBNM problem using a wide variety of prior families $\mathcal{G}$. For some prior
families, it leverages code from existing packages; for others, it
implements new model fitting algorithms with a mind toward speed and
robustness. In this vignette, we demonstrate usage of
the `ebnm` package in an analysis of weighted on-base averages for Major League Baseball players.

## References

<div id="refs"></div>

## Session information

The following R version and packages were used to generate this vignette:

```{r}
sessionInfo()
```

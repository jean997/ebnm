---
title: Extending ebnm with custom ebnm-style functions 
output: 
  rmarkdown::html_vignette:
    toc: yes
vignette: >
  %\VignetteIndexEntry{Extending ebnm with custom ebnm-style functions}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r knitr-opts, include=FALSE}
knitr::opts_chunk$set(comment = "#", collapse = TRUE, results = "hold",
                      fig.align = "center", dpi = 90)
```

The **ebnm** package, in addition to providing implementations of
several commonly used priors (normal, Laplace, etc.), was designed to
be easily extensible so that researchers are not limited by the
existing options (despite the fact that a wide variety of options are
available!).

In this vignette, we illustrate how to extend **ebnm** by creating a
*custom EBNM solver* in the style of other **ebnm** functions such as
`ebnm_normal()` and `ebnm_point_laplace()`. In this illustration, we
implement an EBNM solver for the family of scaled (Student's) *t*
priors. (As of this writing, it is not one of the prior families
included in **ebnm**.) This is formally defined in the next section.

**Please note:** This vignette assumes that you are already familiar
with the basic functionality of the **ebnm** package and have read the
[ebnm paper][ebnm-paper].

### EBNM for the scaled-*t* prior family

The *empirical Bayes normal means* (EBNM) model with scaled-*t* prior
is:

\begin{aligned}
x_i &\sim \mathcal{N}(\theta_i, s_i^2), \\
\theta_i &\sim g \in \mathcal{G}_t,
\end{aligned}

$\mathcal{G}_t$ is the family of scaled-*t* priors and is defined as
follows:

\begin{equation}
\mathcal{G}_t := \{g: g(x) = \sigma t_{\nu}(x); \sigma > 0, \nu > 0\},
\end{equation}

where $t_{\nu}(x)$ denotes the density function of the *t*
distribution at $x$, with $\nu$ degrees of freedom. Fitting the prior
therefore involves estimating two parameters: the scale parameter,
$\sigma$, and the degrees of freedom, $\nu$.

For background and mathematical notation, please see the
[ebnm paper][ebnm-paper].

Overview of the process
-----------------------

Implementing the custom EBNM solver will involve the following steps:

1. Specify the prior family $\mathcal{G}$;

2. Given observations $x_i$ with standard errors $s_i$, estimate $g
\in \mathcal{G}$;

3. Given an estimate $\hat{g} \in \mathcal{G}$, compute posterior
means $\mathbb{E}(\theta_i \mid x_i, s_i, \hat{g})$ and/or other
quantities of interest.

In the following sections, we show how we might implement these three
steps for the prior family $\mathcal{G}_t$. We then incorporate these
implementations into a new function `ebnm_t()` that uses the common
interface supplied by the **ebnm** package. After checking our new
function using the convenient checker `ebnm_check_fn()`, we
demonstrate that, as expected, our new function performs better than
the family of normal priors when the "true means" $\theta_i$ are
simulated from a scaled-*t* distribution.

Apart from using the common **ebnm** interface, we do not have
specific requirements for **ebnm**-style solvers to be incorporated
into the **ebnm** package. In general, however, we prefer that code
styling adheres to the
[Tidyverse style guide](https://style.tidyverse.org/). Further, we
expect functions to be well-tested. At minimum, functions should pass
the tests in `ebnm_check_fn()`. Additional unit tests are strongly
encouraged; **ebnm** runs unit tests on all currently implemented
prior families using the **testthat**
[package](https://testthat.r-lib.org/).

Step 1: define the prior family class
-------------------------------------

First, we define a data structure for the priors in our prior
family. Sometimes, an existing data structure can be used. For
example, `ebnm_normal()`, `ebnm_point_normal()`,
`ebnm_normal_scale_mixture()`, and `ebnm_point_mass()` all share the
`"normalmix"` class. Here we will define a new class.

<!-- which is borrowed from the **ashr** package -->

**ebnm** uses this data structure in two ways: (1) to store information
about the fitted prior, $\hat{g}$ (via the `fitted_g` field in the
returned `"ebnm"` object); (2) to initialize solutions (via the
`g_init` argument).

<!-- If one would like to be able to arbitrarily initialize the
solver, then the data structure should include all information
required to do so. -->

<!-- Note, however, that **ebnm** places no restrictions on data
structure since any initialization using `g_init` must be implemented
within the custom function itself. In particular, we could choose to
simply ignore `g_init` and return `fitted_g = NULL` (though this is
not recommended, since it gives no information about the fitted prior
$\hat{g} \in \mathcal{G}$). -->

For the scaled-*t* prior, we define a new class, `"tdist"`, that
includes the scale and degrees of freedom:

```{r tdist}
tdist <- function (scale, df) {
  structure(data.frame(scale, df), class = "tdist")
}
```

Step 2: implement the optimization function
-------------------------------------------

Next we implement a function for estimating the two parameters
specifying the prior. This estimation is typically done by maximizing
the likelihood. There are many approaches one can take to implement
this step, and the best approach very much depends on context. For a
good overview of the many R packages for numerical optimization,
please see the [CRAN task view on optimization][cran-task-view-opt].

<!-- Most of the EBNM solvers in **ebnm** include an "optmethod"
argument; when estimating parametric priors (e.g., normal or
point-Laplace), this argument allows the user to control and fine-tune
the optimization. One can choose among three general-purpose
optimization methods: a Newton-type algorithm implemented by `nlm()`
function; the L-BFGS-B algorithm implemented by `optim()`; and the
trust region algorithm implemented by the **trust** package. Further,
the user can choose whether to use analytic expressions to evaluate
gradients and/or Hessians or whether to estimate them using numerical
methods. -->

Here, we will use the L-BFGS-B method (implemented by the `optim()`
function) to maximize the log-likelihood. There are at least a couple
reasons why we prefer using L-BFGS-B here: (1) it doesn't require
installing any additional packages; (2) it allows for bound
constraints which is helpful since the two parameters in the prior
both need to be positive. Further, to avoid numerical issues, we set
sensible upper and lower bounds for the parameters: $\min_i s_i / 10
\le \sigma \le \max_i x_i$, $1 \le \nu \le 1000$.

Putting this all together, the optimization function looks like this:

```{r opt_t}
opt_t <- function (x, s, sigma_init, nu_init) {
  optim(
    par = c(sigma_init, nu_init), 
    fn = function (par) -llik_t(x, s, par[1], par[2]), 
    method = "L-BFGS-B",
    lower = c(min(s)/10, 1),
    upper = c(max(x), 1e3)
  )
}
```

This function calls another function, `llik_t()`, which isn't yet
implemented: this function should give us the log-likelihood at the
current parameter estimates. (Note that, since `optim()` attempts to
minimize the objective, we compute the negative log-likelihood.)

Computing log-likelihood involves computing 1-d integrals, or
*1-d convolutions*, over the unknown means $\theta_i$:

\begin{equation}
\log p(\mathbf{x} \mid g, \, \mathbf{s}) = 
\sum_{i=1}^n \textstyle \log
\int p(x_i \mid \theta_i, s_i) \, g(\theta_i) \, d\theta_i.
\end{equation}

Since we do not have a convenient closed-form expression for these
integrals, we compute these integrals numerically using the
`integrate()` function:

<!-- Inside the integral, we have the product of a normal distribution
and the prior, which in our case is a *t* distribution. -->

```{r llik_t}
llik_t <- function (x, s, sigma, nu) {
  llik_one_obs <- function (x, s) {
    integrate(
      f = function (theta) {
        dnorm(x - theta, sd = s) * dt(theta / sigma, df = nu) / sigma
      }, 
      lower = -Inf, 
      upper = Inf
    )$value
  }
  vllik <- Vectorize(llik_one_obs) 
  return(sum(log(vllik(x, s))))
}
```

### Include gradients in the optimization (optional)

**TO DO: Fix this next part.**

In general, we found that it was often fastest to evaluate gradients
analytically but to allow Hessians to be computed numerically. We thus
recommend including gradients when possible.

Notice that we have not included the gradient of the log-likelihood in
our optimization function. Indeed, the gradient is difficult to
compute in this case, so we allow `optim()` to estimate it
numerically. If, however, an efficient expression for computing the
gradient were available (e.g., by explicitly calculating the gradient
or using a package that performs automatic differentiation), we could
implement it as follows (replacing the numerical estimation of the
gradient using **numDeriv** with this more efficient function):

```{r opt_grad}
opt_t_grad <- function(x, s, sigma_init, nu_init) {
  optim(
    par = c(sigma_init, nu_init), 
    fn = function(par) -llik_t(x, s, par[1], par[2]), 
    gr = function(par) -grad_t(x, s, par[1], par[2]),
    method = "L-BFGS-B",
    lower = c(min(s) / 10, 1),
    upper = c(max(x), 1e3)
  )
}

grad_t <- function(x, s, sigma, nu) {
  numDeriv::grad(function(par) llik_t(x, s, par[1], par[2]), c(sigma, nu))
}
```

Step 3: implement the posterior summary function
------------------------------------------------

The other key computation is the computation of summary statistics
(means, variances, etc.) from the posterior distribution.

From Bayes' rule, the posterior distribution for the *i*th unknown
mean is

\begin{equation}
p(\theta_i \mid x_i, s_i, g) \propto 
p(x_i \mid \theta_i, s_i) \, g(\theta_i).
\end{equation}

For this example, we compute three posterior statistics: the posterior
mean, the posterior second moment, and the posterior standard
deviation. This is all accomplished inside one function that returns a
data frame containing the posterior statistics:

```{r post_summary-t}
post_summary_t <- function(x, s, sigma, nu) {
  samp <- post_sampler_t(x, s, sigma, nu, nsamp = 1000)
  return(data.frame(
    mean = colMeans(samp),
    sd = apply(samp, 2, sd),
    second_moment = apply(samp, 2, function(x) mean(x^2))
  ))
}
```

The missing piece is a function `post_sampler_t()` that draws random
samples from the posterior distribution. While drawing independent
samples is difficult, we can easily design an MCMC scheme to
approximately draw samples from the posterior. This is implemented
using the **mcmc** package (which you should install if you haven't
already):

```{r post_sampler_t}
# install.packages("mcmc")
library(mcmc)
post_sampler_t <- function (x, s, sigma, nu, nsamp) {
  sample_one_theta <- function (x_i, s_i) {
    lpostdens <- function (theta) {
      dt(theta/sigma, df = nu, log = TRUE) -
	    log(sigma) + 
        dnorm(x_i - theta, sd = s_i, log = TRUE)
    }
    metrop(lpostdens, initial = x_i, nbatch = nsamp)$batch
  }
  vsampler <- Vectorize(sample_one_theta)
  return(vsampler(x, s))
}
```

This is most certainly not the most efficient way to compute the
posterior summaries. But we do it this way here to keep the example
simple.

<!-- Given an optimal $\hat{g} \in \mathcal{G}$, we can obtain point
estimates for the "true means" $\theta_i$ using posterior means
$\mathbb{E}(\theta_i \mid x_i, s_i, \hat{g})$. In general, we might
like a number of summaries about the posterior. -->

<!-- including, for example, posterior standard deviations, second moments,
and local false sign rates [@Stephens_NewDeal]. If the EBNM solver is
intended for use in the **flashier** package, then posterior means and
second moments *must* be computed. -->

Step 4: put it all together
---------------------------

We have now implemented the key computations for our new EBNM
solver. We will now incorporate these computations into a single
function, ebnm_t, that accepts the same inputs as the other
EBNM-solver functions.

<!-- The inputs should be the same as the other **ebnm** functions. We
will also use the same defaults. We can ignore parameters that are not
relevant (here, `optmethod` and `control`). -->

<!-- The `optmethod` parameter can typically be ignored unless we
want to make multiple optimization methods available for a single
prior family (e.g., both `nlm()` and `optim()`). We will also ignore
the `control` parameter, although it could be used here, for example,
to alter the default settings of `lower` and `upper` in the call to
`optim`. -->

For simplicity, we ignore the "output" parameter and just return all
the results (data, posterior summaries, fitted prior, log-likelihood
and posterior sampler). See `help(ebnm)` for details about the
expected structure of return value.

Here's the new function:

```{r ebnm_t}
ebnm_t <- function (x, 
                    s = 1, 
					mode = 0, 
					scale = "estimate", 
					g_init = NULL, 
					fix_g = FALSE, 
					output = ebnm_output_default(),
					optmethod = NULL,
					control = NULL) {
				   
  # Some basic argument checks.
  if (mode != 0) {
    stop("The mode of the t-prior must be fixed at zero.")
  }
  if (scale != "estimate") {
    stop("The scale of the t-prior must be estimated rather than fixed ",
	     "at a particular value.")
  }
  
  # If g_init is provided, extract the parameters. Otherwise, provide
  # reasonable initial estimates.
  if (!is.null(g_init)) {
    sigma_init <- g_init$scale
    nu_init    <- g_init$df
  } else {
    sigma_init <- sqrt(mean(x^2))
    nu_init    <- 10
  }
  
  # If g is fixed, use g_init. Otherwise optimize g.
  if (fix_g) {
    sigma <- sigma_init
    nu    <- nu_init
    llik  <- llik_t(x, s, sigma, nu)
  } else {
    opt_res <- opt_t(x, s, sigma_init, nu_init)
    sigma   <- opt_res$par[1]
    nu      <- opt_res$par[2]
    llik    <- -opt_res$value
  }
  
  # Prepare the final output.
  return(structure(list(
    data = data.frame(x = x, s = s),
    posterior = post_summary_t(x, s, sigma, nu),
    fitted_g = tdist(scale = sigma, df = nu),
    log_likelihood = llik,
    post_sampler = function (nsamp) post_sampler_t(x, s, sigma, nu, nsamp)
  ), class = c("list", "ebnm")))
}
```

Step 5: verify the EBNM function
--------------------------------

**ebnm** provides a function, `ebnm_check_fn()`, that runs basic tests
to verify that the EBNM function works as expected. Let's run the
checks using a small, simulated data set:

```{r check}
library(ebnm)
set.seed(1)
x <- rnorm(100, sd = 2)
s <- rep(1, 100)
ebnm_check_fn(ebnm_t, x, s)
```

Step 6: the new EBNM function in action
---------------------------------------

Let's illustrate the use of our new EBNM function on a data set in
which the (unobserved) means are simulated from a *t* distribution
with a scale of 2 and 5 degrees of freedom:

```{r sim-data}
set.seed(1)
theta <- 2 * rt(100, df = 5)
x <- theta + rnorm(200)
```

We will compare the use of the *t* prior with a normal prior, which is
implemented by `ebnm_normal()`.

```{r t-vs-normal}
normal_res <- ebnm_normal(x, s = 1)
t_res <- ebnm_t(x, s = 1)
```

<!-- Next we fit EBNM models using: 1. the family of normal priors, as
implemented by `ebnm_normal()`; and 2. the family of *t* priors that
we just implemented. Due to our reliance on MCMC sampling to calculate
posterior means, fitting the second model is much slower than the
first (it took us about 15 seconds on a new MacBook Pro). -->

You may have noticed that the call to `ebnm_t()` took longer than the
call to `ebnm_normal()`. That is expected because the computations
with the *t* distribution are more complex, and we did not put a lot of
effort into making the computations efficient.

Let's compare the two results:

```{r plot-t-vs-normal, fig.width=6, fig.height=4}
plot(normal_res, t_res)
```

Most strikingly, `ebnm_t()` shrinks the large values less aggressively
than `ebnm_normal()`. The fit with the *t* prior also resulted in
slightly more accurate results overall:

```{r rmse}
rmse_normal <- sqrt(mean((coef(normal_res) - theta)^2))
rmse_t <- sqrt(mean((coef(t_res) - theta)^2))
c(rmse_normal = rmse_normal, rmse_t = rmse_t)
```

<!-- This improvement is as expected, since the true prior $g$ used to
simulate the data belongs to the family of *t* distributions but not
the family of normal distributions.-->

Finally, the estimated prior is not far from the parameters used to
simulate the data ($\sigma = 2$, $\nu = 5$):

```{r t-fitted-g}
t_res$fitted_g
```

Session information
-------------------

The following R version and packages were used to generate this
vignette:

```{r session-info}
sessionInfo()
```

[ebnm-paper]: https://arxiv.org/abs/2110.00152
[cran-task-view-opt]: https://cran.r-project.org/web/views/Optimization.html

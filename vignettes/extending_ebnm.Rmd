---
title: Extending ebnm with custom ebnm-style functions 
output: 
  rmarkdown::html_vignette:
    toc: yes
vignette: >
  %\VignetteIndexEntry{Extending ebnm with custom ebnm-style functions}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r knitr-opts, include=FALSE}
knitr::opts_chunk$set(comment = "#", collapse = TRUE, results = "hold",
                      fig.align = "center", dpi = 90)
```

The **ebnm** package, in addition to providing implementations of
several commonly used priors (normal, Laplace, etc.), was
designed to be easily extensible so that researchers are not limited
by the existing options (though a wide variety of options are
available). Here we show by example how to extend **ebnm** by creating
a custom EBNM solver in the style of other **ebnm** functions
(`ebnm_normal()`, `ebnm_point_laplace()`, etc.). Namely, we
will implement a solution to the empirical Bayes normal means problem
using the family of scaled (Student's) *t* priors, which is not currently one of
the prior families included in the **ebnm** package. 

The empirical Bayes normal means (EBNM) model with scaled *t* prior is:

\begin{aligned}
x_i &\sim \mathcal{N}(\theta_i, s_i^2), \\
\theta_i &\sim g \in \mathcal{G}_t,
\end{aligned}

where the prior family $\mathcal{G}_t$ is defined:

\begin{equation}
\mathcal{G}_t := \{g: g = \sigma t_\nu; \sigma > 0, \nu > 0\},
\end{equation}

where $t_{\nu}$ is the *t* distribution with $\nu$ degrees of
freedom. Fitting the prior therefore involves estimating two
parameters: the scale parameter, $\sigma$; and the degrees of freedom,
$\nu$.

For background and notation, 
please refer to the **ebnm** [paper][ebnm-paper].

## Overview

In general, to fit an EBNM model one needs to: 1. Specify the prior family $\mathcal{G}$;
2. Given observations $x_i$ with standard errors $s_i$, estimate
$g \in \mathcal{G}$; 3. Given an estimate $\hat{g} \in \mathcal{G}$, compute
posterior means $\mathbb{E}(\theta_i \mid x_i, s_i, \hat{g})$ and/or other 
quantities of interest. In the following sections, we show how we
might implement these three steps for the prior family $\mathcal{G}_t$. We
then incorporate these implementations into a new function `ebnm_t()` that 
uses the common interface supplied by the **ebnm** package. After checking our
new function using the convenient checker `ebnm_check_fn()`, we 
demonstrate that, as expected, our new function performs better than the family
of normal priors when the "true means" $\theta_i$ are simulated from a scaled
*t* distribution.

Apart from using the common **ebnm** interface, we do not have specific 
requirements for **ebnm**-style solvers to be incorporated into the **ebnm** 
package. In general, however, we prefer that code styling adheres to the 
[Tidyverse style guide](https://style.tidyverse.org/). Further, we expect
functions to be well-tested. At minimum, functions should pass the tests in
`ebnm_check_fn()`. Additional unit tests are strongly encouraged; **ebnm**
runs unit tests on all currently implemented prior families using the 
**testthat** [package](https://testthat.r-lib.org/).

## The prior family class

The first step in implementing a solution to the EBNM problem with scaled
*t* prior is to define a data structure for the priors in our prior
family. In some cases, an existing data structure can be used. For
example, `ebnm_normal()`, `ebnm_point_normal()`,
`ebnm_normal_scale_mixture()`, and `ebnm_point_mass()` all use the
`"normalmix"` class, which is borrowed from the **ashr**
package. Here, we will define a new class.

**ebnm** uses these data structures in two ways: to give information
about the fitted prior $\hat{g}$ (via the `fitted_g` field in the
returned `"ebnm"` object); and to initialize solutions (via the
`g_init` argument). If one would like to be able to arbitrarily
initialize the solver, then the data structure should include all
information required to do so.

<!-- Note, however, that **ebnm** places no restrictions on data
structure since any initialization using `g_init` must be implemented
within the custom function itself. In particular, we could choose to
simply ignore `g_init` and return `fitted_g = NULL` (though this is
not recommended, since it gives no information about the fitted prior
$\hat{g} \in \mathcal{G}$). -->

For the scaled *t* prior, we define a new class, `"tdist"`, that
includes the scale and degrees of freedom:

```{r tdist}
tdist <- function (scale, df) {
  structure(data.frame(scale, df), class = "tdist")
}
```

## The optimization function

Next, we implement a function for estimating the two parameters
specifying the prior. This estimation is typically done by maximizing the 
likelihood. There are many possible ways to perform this maximization; for
an overview of the many R packages available for solving optimization
problems using various algorithms and implementing various constraints, see the 
[CRAN task view on optimization](https://cran.r-project.org/web/views/Optimization.html).
Solvers in the **ebnm** package include an `optmethod` argument; when estimating
parametric priors (e.g., normal or point-Laplace), this argument allows the user to 
choose among three general-purpose optimization methods:
the Newton-type algorithm implemented in the base R `nlm()` function; the 
L-BFGS-B algorithm implemented in the base R `optim()` function; and the trust region 
algorithm implemented in the **trust** package. Further, the user can choose 
whether to use analytic expressions to evaluate gradients and/or Hessians or 
whether to estimate them using numerical methods. In general, we found that it 
was often fastest to evaluate gradients analytically but to allow Hessians to be
computed numerically. We thus recommend including the gradient for the log
likelihood when it is straightforward to do so.

Here, we will use the L-BFGS-B algorithm to maximize the log likelihood since:
1. it doesn't require installing any new packages; and 2. unlike `nlm()`, it allows
us to constrain parameters (recall that $\nu$ and $\sigma$ should always be 
greater than zero). To avoid numerical issues, we set sensible lower
and upper bounds for both parameters (namely, 
$\min_i s_i / 10 \le \sigma \le \max_i x_i$ and $1 \le \nu \le 1000$).
Our optimization function takes initial parameter estimates as input and
returns optimal values for the parameters of interest:

```{r opt_t}
opt_t <- function(x, s, sigma_init, nu_init) {
  optim(
    par = c(sigma_init, nu_init), 
    fn = function(par) -llik_t(x, s, par[1], par[2]), 
    method = "L-BFGS-B",
    lower = c(min(s)/10, 1),
    upper = c(max(x), 1e3)
  )
}
```

Notice that this function calls another function, `llik_t()`, which is
not yet implemented: this function will compute the log likelihood
at the current parameter estimates `par[1]` and `par[2]`. Since `optim()` performs
minimization by default, we take the negative of the log likelihood.

The log likelihood involves an integral or *convolution* over the
unknown means:

\begin{equation}
\log p(\mathbf{x} \mid g, \, \mathbf{s}) = 
\sum_{i=1}^n \textstyle \log
\int p(x_i \mid \theta_i, s_i) \, g(\theta_i) \, d\theta_i
\end{equation}

Inside the integral, we have the product of a normal distribution and
the prior, which in our case is a *t* distribution. We compute this
integral numerically using the built-in `integrate()` function:

```{r llik_t}
llik_t <- function(x, s, sigma, nu) {
  llik_one_obs <- function(x, s) {
    integrate(
      f = function(theta) {
        dnorm(x - theta, sd = s) * dt(theta / sigma, df = nu) / sigma
      }, 
      lower = -Inf, 
      upper = Inf
    )$value
  }
  vllik <- Vectorize(llik_one_obs) 
  return(sum(log(vllik(x, s))))
}
```

Notice that we have not included the gradient of the log likelihood in our
optimization function. Indeed, the gradient is difficult to compute in this case, so
we allow `optim()` to estimate it numerically. If, however, an
efficient expression for computing the gradient were available (e.g., by 
explicitly calculating the gradient or using a package that performs 
automatic differentiation), we could 
implement it as follows (replacing the numerical estimation of the gradient
using **numDeriv** with this more efficient function):

```{r opt_grad}
opt_t_grad <- function(x, s, sigma_init, nu_init) {
  optim(
    par = c(sigma_init, nu_init), 
    fn = function(par) -llik_t(x, s, par[1], par[2]), 
    gr = function(par) -grad_t(x, s, par[1], par[2]),
    method = "L-BFGS-B",
    lower = c(min(s) / 10, 1),
    upper = c(max(x), 1e3)
  )
}

grad_t <- function(x, s, sigma, nu) {
  numDeriv::grad(function(par) llik_t(x, s, par[1], par[2]), c(sigma, nu))
}
```

## The posterior summary function

The other key computation required to fit the EBNM model is the computation of various
statistics (means, variances, etc.) from the posterior distribution of
the "true means" given the prior we estimated. From Bayes' rule, the
posterior distribution for the *i*th unknown mean is

\begin{equation}
p(\theta_i \mid x_i, s_i, g) \propto 
g(\theta_i) \, p(x_i \mid \theta_i, s_i),
\end{equation}

In this example, we summarize the posterior by the mean, standard
deviation and second moment:

```{r post_summary-t}
post_summary_t <- function(x, s, sigma, nu) {
  samp <- post_sampler_t(x, s, sigma, nu, nsamp = 1000)
  return(data.frame(
    mean = colMeans(samp),
    sd = apply(samp, 2, sd),
    second_moment = apply(samp, 2, function(x) mean(x^2))
  ))
}
```

The function requires a function `post_sampler_t` to draw *random
samples* from the posterior distribution. While drawing independent
samples is difficult, we can easily design an MCMC scheme to
approximately draw samples from the posterior. We implement this 
using the **mcmc** package:

```{r post_sampler_t}
# install.packages("mcmc")
library(mcmc)
post_sampler_t <- function(x, s, sigma, nu, nsamp) {
  sample_one_theta <- function(x_i, s_i) {
    lpostdens <- function(theta) {
      dt(theta/sigma, df = nu, log = TRUE) - log(sigma) + 
        dnorm(x_i - theta, sd = s_i, log = TRUE)
    }
    metrop(lpostdens, initial = x_i, nbatch = nsamp)$batch
  }
  vsampler <- Vectorize(sample_one_theta)
  return(vsampler(x, s))
}
```

Note this is most likely not the most efficient way to compute
posterior summaries, but we tried to keep our implementation simple
for purposes of illustration.

<!-- Given an optimal $\hat{g} \in \mathcal{G}$, we can obtain point
estimates for the "true means" $\theta_i$ using posterior means
$\mathbb{E}(\theta_i \mid x_i, s_i, \hat{g})$. In general, we might
like a number of summaries about the posterior. -->

<!-- including, for example, posterior standard deviations, second moments,
and local false sign rates [@Stephens_NewDeal]. If the EBNM solver is
intended for use in the **flashier** package, then posterior means and
second moments *must* be computed. -->

## Putting it all together

We have now implemented the key computations for our new EBNM
solver. We will call this new solver `ebnm_t()` to be consistent
with other **ebnm** functions (`ebnm_normal()`, etc.).

The inputs should be the same as the other **ebnm** functions. We will
also use the same defaults. We can ignore parameters that are not
relevant (here, `optmethod` and `control`).

<!-- The `optmethod` parameter can typically be ignored unless we
want to make multiple optimization methods available for a single
prior family (e.g., both `nlm()` and `optim()`). We will also ignore
the `control` parameter, although it could be used here, for example,
to alter the default settings of `lower` and `upper` in the call to
`optim`. -->

For simplicity, we will ignore the `output` parameter and just return
everything (the data, the posterior summaries, the fitted prior, the
log likelihood, and the posterior sampler). See `help(ebnm)` for
further details about the expected structure of the returned `"ebnm"`
object.

Here's the new function:

```{r ebnm_t}
ebnm_t <- function(x, 
                   s = 1, 
                   mode = 0, 
                   scale = "estimate", 
                   g_init = NULL, 
                   fix_g = FALSE, 
                   output = ebnm_output_default(),
                   optmethod = NULL,
                   control = NULL) {
				   
  # Some very basic argument checks.
  if (mode != 0) {
    stop("The mode of the t-prior must be fixed at zero.")
  }
  if (scale != "estimate") {
    stop("The scale of the t-prior must be estimated rather than fixed ",
	     "at a particular value.")
  }
  
  # If g_init is provided, extract the parameters. Otherwise, provide
  # reasonable initial estimates.
  if (!is.null(g_init)) {
    sigma_init <- g_init$scale
    nu_init    <- g_init$df
  } else {
    sigma_init <- sqrt(mean(x^2))
    nu_init    <- 10
  }
  
  # If g is fixed, use g_init. Otherwise optimize g.
  if (fix_g) {
    sigma <- sigma_init
    nu    <- nu_init
    llik  <- llik_t(x, s, sigma, nu)
  } else {
    opt_res <- opt_t(x, s, sigma_init, nu_init)
    sigma   <- opt_res$par[1]
    nu      <- opt_res$par[2]
    llik    <- -opt_res$value
  }
  
  # Prepare the final output.
  return(structure(list(
    data = data.frame(x = x, s = s),
    posterior = post_summary_t(x, s, sigma, nu),
    fitted_g = tdist(scale = sigma, df = nu),
    log_likelihood = llik,
    post_sampler = function (nsamp) post_sampler_t(x, s, sigma, nu, nsamp)
  ), class = c("list", "ebnm")))
}
```

## Checking the EBNM function

**ebnm** provides a function, `ebnm_check_fn()`, that runs basic tests
to verify that the EBNM function works as expected. Let's run the
checks using a small, simulated data set:

```{r check}
library(ebnm)
set.seed(1)
x <- rnorm(100, sd = 2)
s <- rep(1, 100)
ebnm_check_fn(ebnm_t, x, s)
```

## The new EBNM function in action

Let's illustrate the use of our new EBNM function on a data set in
which the (unobserved) means are simulated from a *t* distribution
with a scale of 2 and 5 degrees of freedom:

```{r sim-data}
set.seed(1)
theta <- 2 * rt(100, df = 5)
x <- theta + rnorm(200)
```

We will compare the use of the *t* prior with a normal prior, which is
implemented by `ebnm_normal()`.

```{r t-vs-normal}
normal_res <- ebnm_normal(x, s = 1)
t_res <- ebnm_t(x, s = 1)
```

<!-- Next we fit EBNM models using: 1. the family of normal priors, as
implemented by `ebnm_normal()`; and 2. the family of $t$ priors that
we just implemented. Due to our reliance on MCMC sampling to calculate
posterior means, fitting the second model is much slower than the
first (it took us about 15 seconds on a new MacBook Pro). -->

You may have noticed that the call to `ebnm_t()` took longer than the
call to `ebnm_normal()`. That is expected because the computations
with the *t* distribution are more complex, and we did not put a lot of
effort into making the computations efficient.

Let's compare the two results:

```{r plot-t-vs-normal, fig.width=6, fig.height=4}
plot(normal_res, t_res)
```

Most strikingly, `ebnm_t()` shrinks the large values less aggressively
than `ebnm_normal()`. The fit with the $t$ prior also resulted in
slightly more accurate results overall:

```{r rmse}
rmse_normal <- sqrt(mean((coef(normal_res) - theta)^2))
rmse_t <- sqrt(mean((coef(t_res) - theta)^2))
c(rmse_normal = rmse_normal, rmse_t = rmse_t)
```

<!-- This improvement is as expected, since the true prior $g$ used to
simulate the data belongs to the family of $t$ distributions but not
the family of normal distributions.-->

Finally, the estimated prior is not far from the parameters used to
simulate the data ($\sigma = 2$, $\nu = 5$):

```{r t-fitted-g}
t_res$fitted_g
```

## Session information

The following R version and packages were used to generate this
vignette:

```{r session-info}
sessionInfo()
```

[ebnm-paper]: https://arxiv.org/abs/2110.00152

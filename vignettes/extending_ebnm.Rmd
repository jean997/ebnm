---
title: Extending ebnm with custom ebnm-style functions 
output: 
  rmarkdown::html_vignette:
    toc: yes
vignette: >
  %\VignetteIndexEntry{Extending ebnm with custom ebnm-style functions}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r knitr-opts, include=FALSE}
knitr::opts_chunk$set(comment = "#", collapse = TRUE, results = "hold",
                      fig.align = "center", dpi = 90)
```

The **ebnm** package, in addition to providing implementations of
several commonly used priors (normal, Laplace, etc.), was
designed to be easily extensible so that researchers are not limited
by the existing options (though a wide variety of options are
available). Here we show by example how to extend **ebnm** by creating
a custom EBNM solver in the style of other **ebnm** functions
(`ebnm_normal()`, `ebnm_point_laplace()`, etc.). In this example, we
will implement a solution to the empirical Bayes normal means problem
using a scaled (Student's) *t* prior, which is not currently one of
the prior families included in the **ebnm** package. 

The EBNM model with scaled *t* prior is

\begin{aligned}
x_i &\sim \mathcal{N}(\theta_i, s_i^2), \\
\theta_i &\sim g \in \mathcal{G}_t,
\end{aligned}

where the prior family $\mathcal{G}_t$ is defined:

\begin{equation}
\mathcal{G}_t := \{g: g = \sigma t_\nu; \sigma > 0, \nu > 0\},
\end{equation}

where $t_{\nu}$ is the *t* distribution with $\nu$ degress of
freedom. Fitting the prior therefore involves estimating two
parameters: the scale parameter, $\sigma$; and the degrees of freedom,
$\nu$.

Please refer to the **ebnm** [paper][ebnm-paper] for background on empirical
Bayes normal means (EBNM) and mathematical notation.

## The prior family class

First, we define a data structure for the priors in our prior
family. In some cases, an existing data structure can be used. For
example, `ebnm_normal()`, `ebnm_point_normal()`,
`ebnm_normal_scale_mixture()`, and `ebnm_point_mass()` all use the
`"normalmix"` class, which is borrowed from the **ashr**
package. Here, we will define a new class.

**ebnm** uses these data structures in two ways: to give information
about the fitted prior $\hat{g}$ (via the `fitted_g` field in the
returned `"ebnm"` object); and to initialize solutions (via the
`g_init` argument). If one would like to be able to arbitrarily
initialize the solver, then the data structure should include all
information required to do so.

<!-- Note, however, that **ebnm** places no restrictions on data
structure since any initialization using `g_init` must be implemented
within the custom function itself. In particular, we could choose to
simply ignore `g_init` and return `fitted_g = NULL` (though this is
not recommended, since it gives no information about the fitted prior
$\hat{g} \in \mathcal{G}$). -->

For the scaled *t* prior, we define a new class, `"tdist"`, that
includes the scale and degrees of freedom:

```{r tdist}
tdist <- function (scale, df) {
  structure(data.frame(scale, df), class = "tdist")
}
```

## The optimization function

Next, we implement a function for estimating the two parameters
specifying the prior. Typically we do this by maximizing the
likelihood. There are several ways one could approach this
maximization problem, but for simplicity we use the `optim()` function
from the stats package since this doesn't require installing any new
packages. In particular, we use `method = "L-BFGS-B"` since it allows
us to constrain the parameters (recall, they should always be greater
than zero). Further, to avoid numerical issues, we set sensible lower
and upper bounds for both parameters:

```{r opt_t}
opt_t <- function(x, s, sigma_init, nu_init) {
  optim(par = c(sigma_init, nu_init), 
        fn = function(par) -llik_t(x, s, par[1], par[2]), 
        method = "L-BFGS-B",
        lower = c(min(s)/10, 1),
        upper = c(max(x), 1e3))
}
```

This function takes the initial parameter estimates as input and and
returns optimal values for the parameters of interest.

Notice that this function calls another function, `llik_t()`, which is
not yet implemented: this function should compute the log-likelihood
at the current parameter estimates. Since `optim()` performs
minimization by default, we take the negative of the log-likelihood.

The log-likelihood involves an *integral* or *convolution* over the
unknown means:

\begin{equation}
\log p(\mathbf{x} \mid g, \, \mathbf{s}) = 
\sum_{i=1}^n \textstyle \log
\int p(x_i \mid \theta_i, s_i) \, g(\theta_i) \, d\theta_i
\end{equation}

Inside the integral, we have the product of a normal distribution and
the prior, which in this case is a *t* distribution. We compute this
integral numerically using the `integrate()` function from the stats
package:

```{r llik_t}
llik_t <- function(x, s, sigma, nu) {
  llik_one_obs <- function(x, s) {
    integrate(f = function(theta) {
                    dnorm(x - theta, sd = s) * dt(theta/sigma, df = nu)/sigma
                  }, lower = -Inf, upper = Inf)$value
  }
  vllik <- Vectorize(llik_one_obs) 
  return(sum(log(vllik(x, s))))
}
```

<!-- 1. Create a function that calculates the log likelihood:

\begin{equation}
L(g) :=  p(\mathbf{x} \mid g, \, \mathbf{s}) = 
\prod_{i=1}^n \textstyle 
\int p(x_i \mid \theta_i, s_i) \, g(\theta_i) \, d\theta_i
\end{equation}

2. Use an off-the-shelf method such as `nlm()` or `optim()` to
   maximize $L(g)$:

\begin{equation}
\hat{g} := 
\text{argmax}_{g \,\in\, \mathcal{G}} L(g)
\end{equation} -->

This completes the prior estimation step.

## The posterior summary function

The other key computation in EBNM is the computation of various
statistics (means, variances, etc.) from the posterior distribution of
the "true means" given the prior we estimated. From Bayes' rule, the
posterior distribution for the *i*th unknown mean is

\begin{equation}
p(\theta_i \mid x_i, s_i, g) \propto 
g(\theta_i) \, p(x_i \mid \theta_i, s_i),
\end{equation}

In this example, we summarize the posterior by the mean, standard
deviation and second moment:

```{r post_summary-t}
post_summary_t <- function(x, s, sigma, nu) {
  samp <- post_sampler_t(x, s, sigma, nu, nsamp = 1000)
  return(data.frame(
    mean = colMeans(samp),
    sd = apply(samp, 2, sd),
    second_moment = apply(samp, 2, function(x) mean(x^2))
  ))
}
```

The function requires a function `post_sampler_t` to draw *random
samples* from the posterior distribution. While drawing independent
samples is difficult, we can easily design an MCMC scheme to
approximately draw samples from the posterior. We implement this 
using the mcmc package:

```{r post_sampler_t}
# install.packages("mcmc")
library(mcmc)
post_sampler_t <- function(x, s, sigma, nu, nsamp) {
  sample_one_theta <- function(x_i, s_i) {
    lpostdens <- function(theta) {
      dt(theta/sigma, df = nu, log = TRUE) - log(sigma) + 
        dnorm(x_i - theta, sd = s_i, log = TRUE)
    }
    metrop(lpostdens, initial = x_i, nbatch = nsamp)$batch
  }
  vsampler <- Vectorize(sample_one_theta)
  return(vsampler(x, s))
}
```

Note this is most likely not the most efficient way to compute
posterior summaries, but we tried to keep our implementation simple
for the purpose of this illustration.

<!-- Given an optimal $\hat{g} \in \mathcal{G}$, we can obtain point
estimates for the "true means" $\theta_i$ using posterior means
$\mathbb{E}(\theta_i \mid x_i, s_i, \hat{g})$. In general, we might
like a number of summaries about the posterior. -->

<!-- including, for example, posterior standard deviations, second moments,
and local false sign rates [@Stephens_NewDeal]. If the EBNM solver is
intended for use in the **flashier** package, then posterior means and
second moments *must* be computed. -->

<!-- Since posteriors are not analytically available for $t$ priors
[or at least, not easily available: see @Pogany], we implement an MCMC
sampler using the **mcmc** package and then compute summaries using
1,000 samples from each posterior. The following code is principally
for purposes of illustration; in particular, speed and efficiency
could likely be improved. -->

## Putting it all together

We have now implemented the key computations for our new EBNM
function. We will call this new function `ebnm_t()` to be consistent
with other EBNM functions (`ebnm_normal()`, etc.).

The inputs should be the same as the other ebnm functions. We will
also use the same defaults. We can ignore parameters that are not
relevant.

<!-- The `optmethod` parameter can typically be ignored unless we
want to make multiple optimization methods available for a single
prior family (e.g., both `nlm()` and `optim()`). We will also ignore
the `control` parameter, although it could be used here, for example,
to alter the default settings of `lower` and `upper` in the call to
`optim`. -->

For simplicity, we will ignore the `output` parameter and just return
everything (the data, the posterior summaries, the fitted prior, the
log likelihood, and the posterior sampler). See `help(ebnm)` for
further details about the expected structure of the returned `"ebnm"`
object.

Here's the new function:

```{r ebnm_t}
ebnm_t <- function(x, 
                   s = 1, 
                   mode = 0, 
                   scale = "estimate", 
                   g_init = NULL, 
                   fix_g = FALSE, 
                   output = ebnm_output_default(),
                   optmethod = NULL,
                   control = NULL) {
				   
  # Some very basic argument checks.
  if (mode != 0) {
    stop("The mode of the t-prior must be fixed at zero.")
  }
  if (scale != "estimate") {
    stop("The scale of the t-prior must be estimated rather than fixed ",
	     "at a particular value.")
  }
  
  # If g_init is provided, extract the parameters, otherwise provide
  # reasonable initial estimates.
  if (!is.null(g_init)) {
    sigma_init <- g_init$scale
    nu_init <- g_init$df
  } else {
    sigma_init <- sqrt(mean(x^2))
    nu_init <- 10
  }
  
  # If g is fixed, use g_init, otherwise optimize g.
  if (fix_g) {
    sigma <- sigma_init
    nu    <- nu_init
    llik  <- llik_t(x, s, sigma, nu)
  } else {
    opt_res <- opt_t(x, s, sigma_init, nu_init)
    sigma   <- opt_res$par[1]
    nu      <- opt_res$par[2]
    llik    <- -opt_res$value
  }
  
  # Prepare the final output.
  return(structure(list(
    data = data.frame(x = x, s = s),
    posterior = post_summary_t(x, s, sigma, nu),
    fitted_g = tdist(scale = sigma, df = nu),
    log_likelihood = llik,
    post_sampler = function (nsamp) post_sampler_t(x, s, sigma, nu, nsamp)
  ), class = c("list", "ebnm")))
}
```

## Checking the EBNM function

**ebnm** provides a function, `ebnm_check_fn()`, that runs basic tests
to verify that the EBNM function works as expected. Let's run the
checks using a small, simulated data set:

```{r check}
library(ebnm)
set.seed(1)
x <- rnorm(100, sd = 2)
s <- rep(1,100)
ebnm_check_fn(ebnm_t, x, s)
```

## The new EBNM function in action

Let's illustrate the use of our new EBNM function on a data set in
which the (unobserved) means are simulated from a *t* distribution
with a scale of 2 and 5 degrees of freedom:

```{r sim-data}
set.seed(1)
theta <- 2 * rt(100, df = 5)
x <- theta + rnorm(200)
```

Let's compare the use of the *t* prior with a normal prior, which is
already implemented by `ebnm_normal()`.

```{r t-vs-normal}
normal_res <- ebnm_normal(x, s = 1)
t_res <- ebnm_t(x, s = 1)
```

<!-- Next we fit EBNM models using: 1. the family of normal priors, as
implemented by `ebnm_normal()`; and 2. the family of $t$ priors that
we just implemented. Due to our reliance on MCMC sampling to calculate
posterior means, fitting the second model is much slower than the
first (it took us about 15 seconds on a new MacBook Pro). -->

You may have noticed that the call to `ebnm_t()` took longer than the
call to `ebnm_normal()`. That is expected because the computations
with the *t* distribution are more complex, and we did not a lot of
effort into making the computations efficient.

Let's compare the two results:

```{r plot-t-vs-normal, fig.width=6, fig.height=4}
plot(normal_res, t_res)
```

Most strikingly, `ebnm_t()` shrinks the large values less aggressively
than `ebnm_normal()`. The fit with the $t$ prior also resulted in
slightly more accurate results overall:

```{r rmse}
rmse_normal <- sqrt(mean((coef(normal_res) - theta)^2))
rmse_t <- sqrt(mean((coef(t_res) - theta)^2))
c(rmse_normal = rmse_normal, rmse_t = rmse_t)
```

<!-- This improvement is as expected, since the true prior $g$ used to
simulate the data belongs to the family of $t$ distributions but not
the family of normal distributions.-->

Also, the estimated prior is not far from the parameters used to
simulate the data ($\sigma = 2$, $\nu = 5$):

```{r t-fitted-g}
t_res$fitted_g
```

## Session information

The following R version and packages were used to generate this
vignette:

```{r}
sessionInfo()
```

[ebnm-paper]: https://arxiv.org/abs/2110.00152

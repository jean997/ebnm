---
title: Extending ebnm with custom ebnm-style functions 
output: 
  rmarkdown::html_vignette:
    toc: yes
bibliography: ebnm.bib
vignette: >
  %\VignetteIndexEntry{Extending ebnm with custom ebnm-style functions}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r knitr-opts, include=FALSE}
knitr::opts_chunk$set(comment = "#", collapse = TRUE, results = "hold",
                      fig.align = "center", dpi = 90)
```

The **ebnm** package, in addition to providing implementations of
several commonly used priors (normal, Laplace, etc.), was also
designed to be easily extensible so that researchers are not limited
by the existing options (though a wide variety of options are
available). Here we show by example how to extend **ebnm** by creating
a custom EBNM solver in the style of other **ebnm** functions
(`ebnm_normal()`, `ebnm_point_laplace()`, etc.). In this example, we
will implement a solution to the empirical Bayes normal means problem
using a scaled (Student's) *t* prior, which is not currently one of
the prior families included in the **ebnm** package.

The EBNM model with scaled *t* prior is

\begin{aligned}
x_i &\sim \mathcal{N}(\theta_i, s_i^2), \\
\theta_i &\sim g \in \mathcal{G}_t,
\end{aligned}

where the prior family $\mathcal{G}_t$ is defined:

\begin{equation}
\mathcal{G}_t := \{g: g = \sigma t_\nu; \sigma > 0, \nu > 0\},
\end{equation}

where $t_{\nu}$ is the *t* distribution with $\nu$ degress of
freedom. Fitting the prior therefore involves estimating two
parameters: the scale parameter, $\sigma$; and the degrees of freedom,
$\nu$.

## The prior family class

First, we define a data structure for the priors in our prior
family. In some cases, an existing data structure can be used. For
example, `ebnm_normal()`, `ebnm_point_normal()`,
`ebnm_normal_scale_mixture()`, and `ebnm_point_mass()` all use the
`"normalmix"` class which is borrowed from the **ashr**
package. Here, we will define a new class.

**ebnm** uses these data structures in two ways: to give information
about the fitted prior, $\hat{g}$ (via the `fitted_g` field in the
returned `"ebnm"` object); and to initialize solutions (via the
`g_init` argument). If one would like to be able to arbitrarily
initialize the solver, then the data structure should include all
information required to do so.

<!-- Note, however, that **ebnm** places no restrictions on data
structure since any initialization using `g_init` must be implemented
within the custom function itself. In particular, we could choose to
simply ignore `g_init` and return `fitted_g = NULL` (though this is
not recommended, since it gives no information about the fitted prior
$\hat{g} \in \mathcal{G}$). -->

For the scaled *t* prior, we define a new class, `"tdist"`, that
includes the scale and degrees of freedom:

```{r tdist}
tdist <- function (scale, df) {
  structure(data.frame(scale, df), class = "tdist")
}
```

## The optimization function

Next, we implement a function for estimating the two parameters
specifying the prior. Typically we do this by maximizing the
likelihood. There are several ways one could approach this
maximization problem, but for simplicity we use the `optim()` function
from the stats package since this doesn't require installing any new
packages. In particular, we use `method = "L-BFGS-B"` since it allows
us to constrain the parameters (recall, they should always be greater
than zero). Further, to avoid numerical issues, we set sensible lower
and upper bounds for both parameters. This is the final optimization
function:

```{r opt_t}
opt_t <- function(x, s, sigma_init, nu_init) {
  optim(par = c(sigma_init, nu_init), 
        fn = function(par) -llik_t(x, s, par[1], par[2]), 
        method = "L-BFGS-B",
        lower = c(min(s)/10, 1),
        upper = c(max(x), 1e3))
}
```

This function takes the initial parameter estimates as input and and
returns optimal values for the parameters of interest.

Notice that this function calls another function, `llik_t()`, which is
not yet implemented: this function should compute the log-likelihood
at the current parameter estimates. Since `optim()` performs
minimization by default, we take the negative of the log-likelihood.

The log-likelihood involves an *integral* or *convolution* over the
unknown mean:

\begin{equation}
\log p(\mathbf{x} \mid g, \, \mathbf{s}) = 
\sum_{i=1}^n \textstyle \log
\int p(x_i \mid \theta_i, s_i) \, g(\theta_i) \, d\theta_i
\end{equation}

Inside the integral, we have the product of a normal distribution and
the prior, which in this case is a *t* distribution. We compute this
integral numerically using the `integrate()` function from the stats
package:

```{r llik_t}
llik_t <- function(x, s, sigma, nu) {
  llik_one_obs <- function(x, s) {
    integrate(f = function(theta) {
                    dnorm(x - theta, sd = s) * dt(theta/sigma, df = nu)/sigma
                  }, lower = -Inf, upper = Inf)$value
  }
  vllik <- Vectorize(llik_one_obs) 
  return(sum(log(vllik(x, s))))
}
```

<!-- 1. Create a function that calculates the log likelihood:

\begin{equation}
L(g) :=  p(\mathbf{x} \mid g, \, \mathbf{s}) = 
\prod_{i=1}^n \textstyle 
\int p(x_i \mid \theta_i, s_i) \, g(\theta_i) \, d\theta_i
\end{equation}

2. Use an off-the-shelf method such as `nlm()` or `optim()` to
   maximize $L(g)$:

\begin{equation}
\hat{g} := 
\text{argmax}_{g \,\in\, \mathcal{G}} L(g)
\end{equation} -->

This completes the prior estimation step.

## Calculating posteriors

Given an optimal $\hat{g} \in \mathcal{G}$, we can obtain point
estimates for the "true means" $\theta_i$ using posterior means
$\mathbb{E}(\theta_i \mid x_i, s_i, \hat{g})$. In general, we might
like a number of summaries about the posterior,

\begin{equation}
p(\theta_i \mid x_i, s_i, \hat{g}) \propto 
\hat{g}(\theta_i) \, p(x_i \mid \theta_i, s_i),
\end{equation}

including, for example, posterior standard deviations, second moments, and local false sign rates [@Stephens_NewDeal]. If the EBNM solver is intended for use in the **flashier** package, then posterior means and second moments *must* be computed.

Since posteriors are not analytically available for $t$ priors [or at least, not easily available: see @Pogany], we implement an MCMC sampler using the **mcmc** package and then compute summaries using 10000 samples from each posterior. The following code is principally for purposes of illustration; in particular, speed and efficiency could likely be improved.

```{r}
post_sampler <- function(x, s, sigma, nu, nsamp) {
  sample_one_theta <- function(x_i, s_i) {
    lpostdens <- function(theta) {
      dt(theta / sigma, df = nu, log = TRUE) - log(sigma) + 
        dnorm(x_i - theta, sd = s_i, log = TRUE)
    }
    mcmc::metrop(lpostdens, initial = x_i, nbatch = nsamp)$batch
  }
  vsampler <- Vectorize(sample_one_theta)
  return(vsampler(x, s))
}

post_summaries <- function(x, s, sigma, nu) {
  samp <- post_sampler(x, s, sigma, nu, nsamp = 10000)
  return(data.frame(
    mean = apply(samp, 2, mean),
    sd = apply(samp, 2, sd),
    second_moment = apply(samp, 2, function(x) mean(x^2))
  ))
}
```


## Putting everything together

We now have all the ingredients required for our custom EBNM solver. We should ensure that inputs are the same as for function `ebnm()` (and we will also use the same defaults). We can simply ignore parameters that we do not want to implement. The `optmethod` parameter can typically be ignored unless we want to make multiple optimization methods available for a single prior family (e.g., both `nlm()` and `optim()`). We will also ignore the `control` parameter, although it could be used here, for example, to alter the default settings of `lower` and `upper` in the call to `optim`. Finally, for simplicity, we will ignore the `output` parameter and just return everything (the data, the posterior summaries, the fitted prior, the log likelihood, and the posterior sampler). See `?ebnm` for further details about the expected structure of the returned `"ebnm"` object.

```{r}
ebnm_t <- function(x, 
                   s = 1, 
                   mode = 0, 
                   scale = "estimate", 
                   g_init = NULL, 
                   fix_g = FALSE, 
                   output = ebnm_output_default(),
                   optmethod = NULL,
                   control = NULL) {
  # Basic argument checks.
  if (mode != 0) {
    stop("The mode of the t-prior must be fixed at zero.")
  }
  if (scale != "estimate") {
    stop("The scale of the t-prior must be estimated rather than fixed at a particular value.")
  }
  
  # If g_init is provided, extract the parameters.
  if (!is.null(g_init)) {
    sigma_init <- g_init$scale
    nu_init <- g_init$df
  } 
  # Otherwise, use sensible defaults.
  else {
    sigma_init <- sqrt(mean(x^2))
    nu_init <- 10
  }
  
  # If g is fixed, use g_init.
  if (fix_g) {
    sigma <- sigma_init
    nu    <- nu_init
    llik  <- llik_t(x, s, sigma, nu)
  } 
  # Otherwise, optimize over the prior family G.
  else {
    opt_res <- opt_t(x, s, sigma_init, nu_init)
    sigma   <- opt_res$par[1]
    nu      <- opt_res$par[2]
    llik    <- -opt_res$value
  }
  
  # Create the return object.
  ebnm_res <- structure(list(
    data = data.frame(x = x, s = s),
    posterior = post_summaries(x, s, sigma, nu),
    fitted_g = tdist(scale = sigma, df = nu),
    log_likelihood = llik,
    post_sampler = function(nsamp) post_sampler(x, s, sigma, nu, nsamp)
  ), class = c("list", "ebnm"))
  
  return(ebnm_res)
}
```


## Checking the function

Conveniently, **ebnm** provides a function `ebnm_check_fn()` that will run basic checks to help determine whether a custom EBNM solver is properly constructed. We can run the check on a random test set as follows:

```{r}
library("ebnm")
ebnm_check_fn(ebnm_t, x = rnorm(100, sd = 2), s = 1)
```


## Illustration using simulated data

Now we are ready to test our function on simulated data. We simulate from a $t$ prior with 5 degrees of freedom and with scale parameter $\sigma = 2$. We then add noise simulated from a standard normal distribution. We set a seed for reproducibility:

```{r}
set.seed(123)
theta <- 2 * rt(100, df = 5)
x <- theta + rnorm(100)
```

Next we fit EBNM models using: 1. the family of normal priors, as implemented by `ebnm_normal()`; and 2. the family of $t$ priors that we just implemented. Due to our reliance on MCMC sampling to calculate posterior means, fitting the second model is much slower than the first (it took us about 15 seconds on a new MacBook Pro). We compare results using the `plot()` method for `"ebnm"` objects:

```{r fig.width=6, fig.height=4}
normal_res <- ebnm_normal(x, s = 1)
t_res <- ebnm_t(x, s = 1)
plot(normal_res, t_res)
```

Note that `ebnm_t()` shrinks smaller observations more aggressively while estimates for larger observations remain close to their original values. As a result, the accuracy (root mean-squared error) of the estimates improves:

```{r}
rmse_normal <- sqrt(mean((coef(normal_res) - theta)^2))
rmse_t <- sqrt(mean((coef(t_res) - theta)^2))
c("rmse_normal" = rmse_normal, "rmse_t" = rmse_t)
```

This improvement is as expected, since the true prior $g$ used to simulate the data belongs to the family of $t$ distributions but not the family of normal distributions. And indeed, the estimated prior parameters end up being similar to the parameters used to simulate the data ($\sigma = 2$ and $\nu = 5$):

```{r}
t_res$fitted_g
```


## Session information

The following R version and packages were used to generate this vignette:

```{r}
sessionInfo()
```


## References
